{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensofrFlow Templates ans some examples\n",
    "\n",
    "### Example 1: Auto Encoder Example.\n",
    "Using an auto encoder on MNIST handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 256\n",
    "display_step = 1\n",
    "examples_to_show = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer num features\n",
    "n_hidden_2 = 128 # 2nd layer num features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([n_input])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                   biases['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
    "                                   biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                   biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
    "                                   biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.211001784\n",
      "Epoch: 0002 cost= 0.173644319\n",
      "Epoch: 0003 cost= 0.159305394\n",
      "Epoch: 0004 cost= 0.145555928\n",
      "Epoch: 0005 cost= 0.138054445\n",
      "Epoch: 0006 cost= 0.130843505\n",
      "Epoch: 0007 cost= 0.125162423\n",
      "Epoch: 0008 cost= 0.121515900\n",
      "Epoch: 0009 cost= 0.120353736\n",
      "Epoch: 0010 cost= 0.120584331\n",
      "Epoch: 0011 cost= 0.115231797\n",
      "Epoch: 0012 cost= 0.113475107\n",
      "Epoch: 0013 cost= 0.108184233\n",
      "Epoch: 0014 cost= 0.107436240\n",
      "Epoch: 0015 cost= 0.103374138\n",
      "Epoch: 0016 cost= 0.100491576\n",
      "Epoch: 0017 cost= 0.099182658\n",
      "Epoch: 0018 cost= 0.097760513\n",
      "Epoch: 0019 cost= 0.099423610\n",
      "Epoch: 0020 cost= 0.094678871\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khalil\\Anaconda3\\lib\\site-packages\\matplotlib\\figure.py:403: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAACNCAYAAACT6v+eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb3a05e0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Launch the graph\n",
    "# Using InteractiveSession (more convenient while using Notebooks)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs})\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "              \"cost=\", \"{:.9f}\".format(c))\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Applying encode and decode over test set\n",
    "encode_decode = sess.run(\n",
    "    y_pred, feed_dict={X: mnist.test.images[:examples_to_show]})\n",
    "# Compare original images with their reconstructions\n",
    "f, a = plt.subplots(2, 10, figsize=(10, 2))\n",
    "for i in range(examples_to_show):\n",
    "    a[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28)))\n",
    "    a[1][i].imshow(np.reshape(encode_decode[i], (28, 28)))\n",
    "f.show()\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: A Convolutional Network implementation example.\n",
    "This example is using the MNIST database of handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, Minibatch Loss= 22780.812500, Training Accuracy= 0.31250\n",
      "Iter 2560, Minibatch Loss= 8774.462891, Training Accuracy= 0.58594\n",
      "Iter 3840, Minibatch Loss= 4503.740234, Training Accuracy= 0.75000\n",
      "Iter 5120, Minibatch Loss= 2808.108887, Training Accuracy= 0.83594\n",
      "Iter 6400, Minibatch Loss= 4636.612793, Training Accuracy= 0.81250\n",
      "Iter 7680, Minibatch Loss= 2380.326416, Training Accuracy= 0.79688\n",
      "Iter 8960, Minibatch Loss= 3798.596436, Training Accuracy= 0.83594\n",
      "Iter 10240, Minibatch Loss= 2178.694580, Training Accuracy= 0.87500\n",
      "Iter 11520, Minibatch Loss= 3760.132812, Training Accuracy= 0.82031\n",
      "Iter 12800, Minibatch Loss= 1930.954834, Training Accuracy= 0.85938\n",
      "Iter 14080, Minibatch Loss= 2230.638672, Training Accuracy= 0.90625\n",
      "Iter 15360, Minibatch Loss= 1975.840698, Training Accuracy= 0.89062\n",
      "Iter 16640, Minibatch Loss= 1385.140503, Training Accuracy= 0.92188\n",
      "Iter 17920, Minibatch Loss= 2228.737793, Training Accuracy= 0.89062\n",
      "Iter 19200, Minibatch Loss= 1858.968384, Training Accuracy= 0.90625\n",
      "Iter 20480, Minibatch Loss= 1619.901611, Training Accuracy= 0.92188\n",
      "Iter 21760, Minibatch Loss= 1405.294312, Training Accuracy= 0.92969\n",
      "Iter 23040, Minibatch Loss= 1846.726562, Training Accuracy= 0.89062\n",
      "Iter 24320, Minibatch Loss= 1048.747070, Training Accuracy= 0.93750\n",
      "Iter 25600, Minibatch Loss= 810.479675, Training Accuracy= 0.93750\n",
      "Iter 26880, Minibatch Loss= 1573.742432, Training Accuracy= 0.89844\n",
      "Iter 28160, Minibatch Loss= 788.763245, Training Accuracy= 0.94531\n",
      "Iter 29440, Minibatch Loss= 2123.495850, Training Accuracy= 0.91406\n",
      "Iter 30720, Minibatch Loss= 1232.617554, Training Accuracy= 0.92188\n",
      "Iter 32000, Minibatch Loss= 971.439819, Training Accuracy= 0.94531\n",
      "Iter 33280, Minibatch Loss= 1352.970215, Training Accuracy= 0.92969\n",
      "Iter 34560, Minibatch Loss= 973.410339, Training Accuracy= 0.95312\n",
      "Iter 35840, Minibatch Loss= 1271.263184, Training Accuracy= 0.92969\n",
      "Iter 37120, Minibatch Loss= 2304.182129, Training Accuracy= 0.86719\n",
      "Iter 38400, Minibatch Loss= 1492.649414, Training Accuracy= 0.93750\n",
      "Iter 39680, Minibatch Loss= 1755.338745, Training Accuracy= 0.89844\n",
      "Iter 40960, Minibatch Loss= 969.823608, Training Accuracy= 0.91406\n",
      "Iter 42240, Minibatch Loss= 893.286987, Training Accuracy= 0.92969\n",
      "Iter 43520, Minibatch Loss= 1087.825928, Training Accuracy= 0.95312\n",
      "Iter 44800, Minibatch Loss= 687.736755, Training Accuracy= 0.91406\n",
      "Iter 46080, Minibatch Loss= 446.910126, Training Accuracy= 0.95312\n",
      "Iter 47360, Minibatch Loss= 456.936493, Training Accuracy= 0.95312\n",
      "Iter 48640, Minibatch Loss= 1578.058594, Training Accuracy= 0.89844\n",
      "Iter 49920, Minibatch Loss= 534.682129, Training Accuracy= 0.94531\n",
      "Iter 51200, Minibatch Loss= 631.263916, Training Accuracy= 0.96094\n",
      "Iter 52480, Minibatch Loss= 581.185425, Training Accuracy= 0.92969\n",
      "Iter 53760, Minibatch Loss= 591.061401, Training Accuracy= 0.95312\n",
      "Iter 55040, Minibatch Loss= 771.995850, Training Accuracy= 0.94531\n",
      "Iter 56320, Minibatch Loss= 780.028137, Training Accuracy= 0.94531\n",
      "Iter 57600, Minibatch Loss= 1280.939941, Training Accuracy= 0.91406\n",
      "Iter 58880, Minibatch Loss= 848.281250, Training Accuracy= 0.94531\n",
      "Iter 60160, Minibatch Loss= 229.848358, Training Accuracy= 0.98438\n",
      "Iter 61440, Minibatch Loss= 467.041534, Training Accuracy= 0.95312\n",
      "Iter 62720, Minibatch Loss= 441.739441, Training Accuracy= 0.97656\n",
      "Iter 64000, Minibatch Loss= 407.643433, Training Accuracy= 0.96094\n",
      "Iter 65280, Minibatch Loss= 550.971924, Training Accuracy= 0.95312\n",
      "Iter 66560, Minibatch Loss= 472.463013, Training Accuracy= 0.95312\n",
      "Iter 67840, Minibatch Loss= 449.810669, Training Accuracy= 0.97656\n",
      "Iter 69120, Minibatch Loss= 315.833679, Training Accuracy= 0.93750\n",
      "Iter 70400, Minibatch Loss= 333.219269, Training Accuracy= 0.97656\n",
      "Iter 71680, Minibatch Loss= 849.106445, Training Accuracy= 0.94531\n",
      "Iter 72960, Minibatch Loss= 310.791412, Training Accuracy= 0.93750\n",
      "Iter 74240, Minibatch Loss= 267.367981, Training Accuracy= 0.95312\n",
      "Iter 75520, Minibatch Loss= 693.844177, Training Accuracy= 0.94531\n",
      "Iter 76800, Minibatch Loss= 472.412964, Training Accuracy= 0.96875\n",
      "Iter 78080, Minibatch Loss= 515.684326, Training Accuracy= 0.96094\n",
      "Iter 79360, Minibatch Loss= 268.306183, Training Accuracy= 0.96875\n",
      "Iter 80640, Minibatch Loss= 805.772888, Training Accuracy= 0.92188\n",
      "Iter 81920, Minibatch Loss= 643.700562, Training Accuracy= 0.92969\n",
      "Iter 83200, Minibatch Loss= 1193.736450, Training Accuracy= 0.93750\n",
      "Iter 84480, Minibatch Loss= 198.591415, Training Accuracy= 0.95312\n",
      "Iter 85760, Minibatch Loss= 512.105286, Training Accuracy= 0.96094\n",
      "Iter 87040, Minibatch Loss= 126.764053, Training Accuracy= 0.96875\n",
      "Iter 88320, Minibatch Loss= 444.921600, Training Accuracy= 0.95312\n",
      "Iter 89600, Minibatch Loss= 429.155090, Training Accuracy= 0.95312\n",
      "Iter 90880, Minibatch Loss= 912.454834, Training Accuracy= 0.92969\n",
      "Iter 92160, Minibatch Loss= 494.557465, Training Accuracy= 0.96094\n",
      "Iter 93440, Minibatch Loss= 121.501968, Training Accuracy= 0.98438\n",
      "Iter 94720, Minibatch Loss= 405.613281, Training Accuracy= 0.93750\n",
      "Iter 96000, Minibatch Loss= 665.598328, Training Accuracy= 0.94531\n",
      "Iter 97280, Minibatch Loss= 509.206299, Training Accuracy= 0.95312\n",
      "Iter 98560, Minibatch Loss= 236.243240, Training Accuracy= 0.97656\n",
      "Iter 99840, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 101120, Minibatch Loss= 284.532104, Training Accuracy= 0.97656\n",
      "Iter 102400, Minibatch Loss= 275.426056, Training Accuracy= 0.97656\n",
      "Iter 103680, Minibatch Loss= 563.667236, Training Accuracy= 0.95312\n",
      "Iter 104960, Minibatch Loss= 441.419586, Training Accuracy= 0.96875\n",
      "Iter 106240, Minibatch Loss= 597.800720, Training Accuracy= 0.93750\n",
      "Iter 107520, Minibatch Loss= 520.680176, Training Accuracy= 0.92969\n",
      "Iter 108800, Minibatch Loss= 373.210144, Training Accuracy= 0.95312\n",
      "Iter 110080, Minibatch Loss= 620.664917, Training Accuracy= 0.95312\n",
      "Iter 111360, Minibatch Loss= 240.053848, Training Accuracy= 0.96875\n",
      "Iter 112640, Minibatch Loss= 216.197647, Training Accuracy= 0.94531\n",
      "Iter 113920, Minibatch Loss= 310.112610, Training Accuracy= 0.96875\n",
      "Iter 115200, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 116480, Minibatch Loss= 69.431641, Training Accuracy= 0.98438\n",
      "Iter 117760, Minibatch Loss= 432.739441, Training Accuracy= 0.96875\n",
      "Iter 119040, Minibatch Loss= 282.635681, Training Accuracy= 0.95312\n",
      "Iter 120320, Minibatch Loss= 6.798157, Training Accuracy= 0.99219\n",
      "Iter 121600, Minibatch Loss= 164.998215, Training Accuracy= 0.98438\n",
      "Iter 122880, Minibatch Loss= 166.261337, Training Accuracy= 0.97656\n",
      "Iter 124160, Minibatch Loss= 67.445633, Training Accuracy= 0.97656\n",
      "Iter 125440, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 126720, Minibatch Loss= 326.447632, Training Accuracy= 0.96875\n",
      "Iter 128000, Minibatch Loss= 194.761536, Training Accuracy= 0.96875\n",
      "Iter 129280, Minibatch Loss= 451.200195, Training Accuracy= 0.94531\n",
      "Iter 130560, Minibatch Loss= 193.017578, Training Accuracy= 0.96094\n",
      "Iter 131840, Minibatch Loss= 182.348663, Training Accuracy= 0.95312\n",
      "Iter 133120, Minibatch Loss= 42.359627, Training Accuracy= 0.97656\n",
      "Iter 134400, Minibatch Loss= 98.480148, Training Accuracy= 0.98438\n",
      "Iter 135680, Minibatch Loss= 102.878838, Training Accuracy= 0.96094\n",
      "Iter 136960, Minibatch Loss= 33.214066, Training Accuracy= 0.98438\n",
      "Iter 138240, Minibatch Loss= 253.626099, Training Accuracy= 0.97656\n",
      "Iter 139520, Minibatch Loss= 173.980225, Training Accuracy= 0.97656\n",
      "Iter 140800, Minibatch Loss= 360.539429, Training Accuracy= 0.96875\n",
      "Iter 142080, Minibatch Loss= 73.184357, Training Accuracy= 0.99219\n",
      "Iter 143360, Minibatch Loss= 563.626892, Training Accuracy= 0.95312\n",
      "Iter 144640, Minibatch Loss= 155.873932, Training Accuracy= 0.96875\n",
      "Iter 145920, Minibatch Loss= 49.706741, Training Accuracy= 0.98438\n",
      "Iter 147200, Minibatch Loss= 175.850632, Training Accuracy= 0.97656\n",
      "Iter 148480, Minibatch Loss= 432.521240, Training Accuracy= 0.95312\n",
      "Iter 149760, Minibatch Loss= 175.858429, Training Accuracy= 0.96875\n",
      "Iter 151040, Minibatch Loss= 117.961647, Training Accuracy= 0.98438\n",
      "Iter 152320, Minibatch Loss= 360.902100, Training Accuracy= 0.95312\n",
      "Iter 153600, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 154880, Minibatch Loss= 125.691208, Training Accuracy= 0.97656\n",
      "Iter 156160, Minibatch Loss= 267.040283, Training Accuracy= 0.97656\n",
      "Iter 157440, Minibatch Loss= 376.995331, Training Accuracy= 0.95312\n",
      "Iter 158720, Minibatch Loss= 103.607735, Training Accuracy= 0.97656\n",
      "Iter 160000, Minibatch Loss= 283.737457, Training Accuracy= 0.96094\n",
      "Iter 161280, Minibatch Loss= 285.103149, Training Accuracy= 0.96094\n",
      "Iter 162560, Minibatch Loss= 524.971375, Training Accuracy= 0.94531\n",
      "Iter 163840, Minibatch Loss= 341.354980, Training Accuracy= 0.95312\n",
      "Iter 165120, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 166400, Minibatch Loss= 81.248817, Training Accuracy= 0.96875\n",
      "Iter 167680, Minibatch Loss= 50.882401, Training Accuracy= 0.97656\n",
      "Iter 168960, Minibatch Loss= 410.486877, Training Accuracy= 0.95312\n",
      "Iter 170240, Minibatch Loss= 309.431213, Training Accuracy= 0.96094\n",
      "Iter 171520, Minibatch Loss= 342.952759, Training Accuracy= 0.96094\n",
      "Iter 172800, Minibatch Loss= 68.847412, Training Accuracy= 0.98438\n",
      "Iter 174080, Minibatch Loss= 32.201157, Training Accuracy= 0.98438\n",
      "Iter 175360, Minibatch Loss= 156.205612, Training Accuracy= 0.96875\n",
      "Iter 176640, Minibatch Loss= 57.691429, Training Accuracy= 0.97656\n",
      "Iter 177920, Minibatch Loss= 205.638657, Training Accuracy= 0.99219\n",
      "Iter 179200, Minibatch Loss= 175.392548, Training Accuracy= 0.96875\n",
      "Iter 180480, Minibatch Loss= 119.314804, Training Accuracy= 0.96875\n",
      "Iter 181760, Minibatch Loss= 490.955994, Training Accuracy= 0.94531\n",
      "Iter 183040, Minibatch Loss= 83.402687, Training Accuracy= 0.96875\n",
      "Iter 184320, Minibatch Loss= 220.734818, Training Accuracy= 0.96875\n",
      "Iter 185600, Minibatch Loss= 84.369087, Training Accuracy= 0.97656\n",
      "Iter 186880, Minibatch Loss= 17.841202, Training Accuracy= 0.98438\n",
      "Iter 188160, Minibatch Loss= 59.133781, Training Accuracy= 0.98438\n",
      "Iter 189440, Minibatch Loss= 186.988464, Training Accuracy= 0.97656\n",
      "Iter 190720, Minibatch Loss= 182.061127, Training Accuracy= 0.97656\n",
      "Iter 192000, Minibatch Loss= 169.819733, Training Accuracy= 0.96875\n",
      "Iter 193280, Minibatch Loss= 81.117615, Training Accuracy= 0.98438\n",
      "Iter 194560, Minibatch Loss= 232.205536, Training Accuracy= 0.96875\n",
      "Iter 195840, Minibatch Loss= 109.775970, Training Accuracy= 0.96094\n",
      "Iter 197120, Minibatch Loss= 65.287094, Training Accuracy= 0.98438\n",
      "Iter 198400, Minibatch Loss= 204.523285, Training Accuracy= 0.97656\n",
      "Iter 199680, Minibatch Loss= 150.304672, Training Accuracy= 0.96094\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.988281\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "                                       keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y,\n",
    "                                                              keep_prob: 1.})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "                                      y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: A Reccurent Neural Network example.\n",
    "This example is using the MNIST database of handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, Minibatch Loss= 1.927748, Training Accuracy= 0.29688\n",
      "Iter 2560, Minibatch Loss= 1.637397, Training Accuracy= 0.44531\n",
      "Iter 3840, Minibatch Loss= 1.372385, Training Accuracy= 0.54688\n",
      "Iter 5120, Minibatch Loss= 0.942392, Training Accuracy= 0.66406\n",
      "Iter 6400, Minibatch Loss= 0.787281, Training Accuracy= 0.71094\n",
      "Iter 7680, Minibatch Loss= 0.833657, Training Accuracy= 0.70312\n",
      "Iter 8960, Minibatch Loss= 0.539212, Training Accuracy= 0.82812\n",
      "Iter 10240, Minibatch Loss= 0.718743, Training Accuracy= 0.75781\n",
      "Iter 11520, Minibatch Loss= 0.531502, Training Accuracy= 0.83594\n",
      "Iter 12800, Minibatch Loss= 0.440251, Training Accuracy= 0.88281\n",
      "Iter 14080, Minibatch Loss= 0.393589, Training Accuracy= 0.87500\n",
      "Iter 15360, Minibatch Loss= 0.284322, Training Accuracy= 0.93750\n",
      "Iter 16640, Minibatch Loss= 0.406558, Training Accuracy= 0.85938\n",
      "Iter 17920, Minibatch Loss= 0.372072, Training Accuracy= 0.85156\n",
      "Iter 19200, Minibatch Loss= 0.392870, Training Accuracy= 0.85938\n",
      "Iter 20480, Minibatch Loss= 0.322351, Training Accuracy= 0.87500\n",
      "Iter 21760, Minibatch Loss= 0.393679, Training Accuracy= 0.89844\n",
      "Iter 23040, Minibatch Loss= 0.319985, Training Accuracy= 0.92188\n",
      "Iter 24320, Minibatch Loss= 0.268279, Training Accuracy= 0.92188\n",
      "Iter 25600, Minibatch Loss= 0.215490, Training Accuracy= 0.93750\n",
      "Iter 26880, Minibatch Loss= 0.238612, Training Accuracy= 0.90625\n",
      "Iter 28160, Minibatch Loss= 0.228738, Training Accuracy= 0.91406\n",
      "Iter 29440, Minibatch Loss= 0.460669, Training Accuracy= 0.87500\n",
      "Iter 30720, Minibatch Loss= 0.145527, Training Accuracy= 0.93750\n",
      "Iter 32000, Minibatch Loss= 0.167097, Training Accuracy= 0.94531\n",
      "Iter 33280, Minibatch Loss= 0.309437, Training Accuracy= 0.92188\n",
      "Iter 34560, Minibatch Loss= 0.189097, Training Accuracy= 0.93750\n",
      "Iter 35840, Minibatch Loss= 0.310133, Training Accuracy= 0.89844\n",
      "Iter 37120, Minibatch Loss= 0.316767, Training Accuracy= 0.92188\n",
      "Iter 38400, Minibatch Loss= 0.339121, Training Accuracy= 0.88281\n",
      "Iter 39680, Minibatch Loss= 0.115036, Training Accuracy= 0.96875\n",
      "Iter 40960, Minibatch Loss= 0.158316, Training Accuracy= 0.96875\n",
      "Iter 42240, Minibatch Loss= 0.149395, Training Accuracy= 0.94531\n",
      "Iter 43520, Minibatch Loss= 0.183580, Training Accuracy= 0.95312\n",
      "Iter 44800, Minibatch Loss= 0.231465, Training Accuracy= 0.92188\n",
      "Iter 46080, Minibatch Loss= 0.127710, Training Accuracy= 0.96875\n",
      "Iter 47360, Minibatch Loss= 0.106919, Training Accuracy= 0.95312\n",
      "Iter 48640, Minibatch Loss= 0.163511, Training Accuracy= 0.96094\n",
      "Iter 49920, Minibatch Loss= 0.157814, Training Accuracy= 0.94531\n",
      "Iter 51200, Minibatch Loss= 0.149977, Training Accuracy= 0.94531\n",
      "Iter 52480, Minibatch Loss= 0.059613, Training Accuracy= 0.98438\n",
      "Iter 53760, Minibatch Loss= 0.191751, Training Accuracy= 0.93750\n",
      "Iter 55040, Minibatch Loss= 0.180139, Training Accuracy= 0.95312\n",
      "Iter 56320, Minibatch Loss= 0.086927, Training Accuracy= 0.97656\n",
      "Iter 57600, Minibatch Loss= 0.155203, Training Accuracy= 0.93750\n",
      "Iter 58880, Minibatch Loss= 0.122371, Training Accuracy= 0.96094\n",
      "Iter 60160, Minibatch Loss= 0.162266, Training Accuracy= 0.95312\n",
      "Iter 61440, Minibatch Loss= 0.070047, Training Accuracy= 0.99219\n",
      "Iter 62720, Minibatch Loss= 0.129942, Training Accuracy= 0.96094\n",
      "Iter 64000, Minibatch Loss= 0.133276, Training Accuracy= 0.96875\n",
      "Iter 65280, Minibatch Loss= 0.124468, Training Accuracy= 0.96875\n",
      "Iter 66560, Minibatch Loss= 0.126989, Training Accuracy= 0.96094\n",
      "Iter 67840, Minibatch Loss= 0.259808, Training Accuracy= 0.95312\n",
      "Iter 69120, Minibatch Loss= 0.109890, Training Accuracy= 0.96094\n",
      "Iter 70400, Minibatch Loss= 0.091982, Training Accuracy= 0.96094\n",
      "Iter 71680, Minibatch Loss= 0.047019, Training Accuracy= 0.99219\n",
      "Iter 72960, Minibatch Loss= 0.084508, Training Accuracy= 0.97656\n",
      "Iter 74240, Minibatch Loss= 0.081405, Training Accuracy= 0.96094\n",
      "Iter 75520, Minibatch Loss= 0.187743, Training Accuracy= 0.93750\n",
      "Iter 76800, Minibatch Loss= 0.128654, Training Accuracy= 0.96094\n",
      "Iter 78080, Minibatch Loss= 0.302759, Training Accuracy= 0.94531\n",
      "Iter 79360, Minibatch Loss= 0.151986, Training Accuracy= 0.95312\n",
      "Iter 80640, Minibatch Loss= 0.119755, Training Accuracy= 0.96094\n",
      "Iter 81920, Minibatch Loss= 0.053232, Training Accuracy= 0.99219\n",
      "Iter 83200, Minibatch Loss= 0.080625, Training Accuracy= 0.98438\n",
      "Iter 84480, Minibatch Loss= 0.147636, Training Accuracy= 0.94531\n",
      "Iter 85760, Minibatch Loss= 0.103256, Training Accuracy= 0.96875\n",
      "Iter 87040, Minibatch Loss= 0.197768, Training Accuracy= 0.95312\n",
      "Iter 88320, Minibatch Loss= 0.188443, Training Accuracy= 0.95312\n",
      "Iter 89600, Minibatch Loss= 0.094118, Training Accuracy= 0.96875\n",
      "Iter 90880, Minibatch Loss= 0.103146, Training Accuracy= 0.96094\n",
      "Iter 92160, Minibatch Loss= 0.113996, Training Accuracy= 0.96875\n",
      "Iter 93440, Minibatch Loss= 0.153196, Training Accuracy= 0.95312\n",
      "Iter 94720, Minibatch Loss= 0.104788, Training Accuracy= 0.96094\n",
      "Iter 96000, Minibatch Loss= 0.056370, Training Accuracy= 0.98438\n",
      "Iter 97280, Minibatch Loss= 0.077152, Training Accuracy= 0.97656\n",
      "Iter 98560, Minibatch Loss= 0.120935, Training Accuracy= 0.96875\n",
      "Iter 99840, Minibatch Loss= 0.139749, Training Accuracy= 0.96094\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.976563\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Biaxial Recurrent Neural Network example.\n",
    "Generating music using neural Networks \" The first section and the model is  based on Daniel Johnson's midi manipulation code in https://github.com/hexahedria/biaxial-rnn-music-composition \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import midi\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "lowerBound = 24\n",
    "upperBound = 102\n",
    "span = upperBound-lowerBound\n",
    "\n",
    "\n",
    "def midiToNoteStateMatrix(midifile, squash=True, span=span):\n",
    "    pattern = midi.read_midifile(midifile)\n",
    "\n",
    "    timeleft = [track[0].tick for track in pattern]\n",
    "\n",
    "    posns = [0 for track in pattern]\n",
    "\n",
    "    statematrix = []\n",
    "    time = 0\n",
    "\n",
    "    state = [[0,0] for x in range(span)]\n",
    "    statematrix.append(state)\n",
    "    condition = True\n",
    "    while condition:\n",
    "        if time % (pattern.resolution / 4) == (pattern.resolution / 8):\n",
    "            # Crossed a note boundary. Create a new state, defaulting to holding notes\n",
    "            oldstate = state\n",
    "            state = [[oldstate[x][0],0] for x in range(span)]\n",
    "            statematrix.append(state)\n",
    "        for i in range(len(timeleft)): #For each track\n",
    "            if not condition:\n",
    "                break\n",
    "            while timeleft[i] == 0:\n",
    "                track = pattern[i]\n",
    "                pos = posns[i]\n",
    "\n",
    "                evt = track[pos]\n",
    "                if isinstance(evt, midi.NoteEvent):\n",
    "                    if (evt.pitch < lowerBound) or (evt.pitch >= upperBound):\n",
    "                        pass\n",
    "                        # print \"Note {} at time {} out of bounds (ignoring)\".format(evt.pitch, time)\n",
    "                    else:\n",
    "                        if isinstance(evt, midi.NoteOffEvent) or evt.velocity == 0:\n",
    "                            state[evt.pitch-lowerBound] = [0, 0]\n",
    "                        else:\n",
    "                            state[evt.pitch-lowerBound] = [1, 1]\n",
    "                elif isinstance(evt, midi.TimeSignatureEvent):\n",
    "                    if evt.numerator not in (2, 4):\n",
    "                        # We don't want to worry about non-4 time signatures. Bail early!\n",
    "                        # print \"Found time signature event {}. Bailing!\".format(evt)\n",
    "                        out =  statematrix\n",
    "                        condition = False\n",
    "                        break\n",
    "                try:\n",
    "                    timeleft[i] = track[pos + 1].tick\n",
    "                    posns[i] += 1\n",
    "                except IndexError:\n",
    "                    timeleft[i] = None\n",
    "\n",
    "            if timeleft[i] is not None:\n",
    "                timeleft[i] -= 1\n",
    "\n",
    "        if all(t is None for t in timeleft):\n",
    "            break\n",
    "\n",
    "        time += 1\n",
    "\n",
    "    S = np.array(statematrix)\n",
    "    statematrix = np.hstack((S[:, :, 0], S[:, :, 1]))\n",
    "    statematrix = np.asarray(statematrix).tolist()\n",
    "    return statematrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noteStateMatrixToMidi(statematrix, name=\"example\", span=span):\n",
    "    statematrix = np.array(statematrix)\n",
    "    if not len(statematrix.shape) == 3:\n",
    "        statematrix = np.dstack((statematrix[:, :span], statematrix[:, span:]))\n",
    "    statematrix = np.asarray(statematrix)\n",
    "    pattern = midi.Pattern()\n",
    "    track = midi.Track()\n",
    "    pattern.append(track)\n",
    "    \n",
    "    span = upperBound-lowerBound\n",
    "    tickscale = 55\n",
    "    \n",
    "    lastcmdtime = 0\n",
    "    prevstate = [[0,0] for x in range(span)]\n",
    "    for time, state in enumerate(statematrix + [prevstate[:]]):  \n",
    "        offNotes = []\n",
    "        onNotes = []\n",
    "        for i in range(span):\n",
    "            n = state[i]\n",
    "            p = prevstate[i]\n",
    "            if p[0] == 1:\n",
    "                if n[0] == 0:\n",
    "                    offNotes.append(i)\n",
    "                elif n[1] == 1:\n",
    "                    offNotes.append(i)\n",
    "                    onNotes.append(i)\n",
    "            elif n[0] == 1:\n",
    "                onNotes.append(i)\n",
    "        for note in offNotes:\n",
    "            track.append(midi.NoteOffEvent(tick=(time-lastcmdtime)*tickscale, pitch=note+lowerBound))\n",
    "            lastcmdtime = time\n",
    "        for note in onNotes:\n",
    "            track.append(midi.NoteOnEvent(tick=(time-lastcmdtime)*tickscale, velocity=40, pitch=note+lowerBound))\n",
    "            lastcmdtime = time\n",
    "            \n",
    "        prevstate = state\n",
    "    \n",
    "    eot = midi.EndOfTrackEvent(tick=1)\n",
    "    track.append(eot)\n",
    "\n",
    "    midi.write_midifile(\"{}.mid\".format(name), pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import msgpack\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tqdm import tqdm\n",
    "\n",
    "###################################################\n",
    "# Should be in the same dir as the next section in order to work\n",
    "\n",
    "import midi_manipulation\n",
    "\n",
    "def get_songs(path):\n",
    "    files = glob.glob('{}/*.mid*'.format(path))\n",
    "    songs = []\n",
    "    for f in tqdm(files):\n",
    "        try:\n",
    "            song = np.array(midi_manipulation.midiToNoteStateMatrix(f))\n",
    "            if np.array(song).shape[0] > 50:\n",
    "                songs.append(song)\n",
    "        except Exception as e:\n",
    "            raise e           \n",
    "    return songs\n",
    "\n",
    "songs = get_songs('Pop_Music_Midi') #These songs have already been converted from midi to msgpack\n",
    "print \"{} songs processed\".format(len(songs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### HyperParameters\n",
    "# First, let's take a look at the hyperparameters of our model:\n",
    "\n",
    "lowest_note = midi_manipulation.lowerBound #the index of the lowest note on the piano roll\n",
    "highest_note = midi_manipulation.upperBound #the index of the highest note on the piano roll\n",
    "note_range = highest_note-lowest_note #the note range\n",
    "\n",
    "num_timesteps  = 15 #This is the number of timesteps that we will create at a time\n",
    "n_visible      = 2*note_range*num_timesteps #This is the size of the visible layer. \n",
    "n_hidden       = 50 #This is the size of the hidden layer\n",
    "\n",
    "num_epochs = 200 #The number of training epochs that we are going to run. For each epoch we go through the entire data set.\n",
    "batch_size = 100 #The number of training examples that we are going to send through the RBM at a time. \n",
    "lr         = tf.constant(0.005, tf.float32) #The learning rate of our model\n",
    "\n",
    "### Variables:\n",
    "# Next, let's look at the variables we're going to use:\n",
    "\n",
    "x  = tf.placeholder(tf.float32, [None, n_visible], name=\"x\") #The placeholder variable that holds our data\n",
    "W  = tf.Variable(tf.random_normal([n_visible, n_hidden], 0.01), name=\"W\") #The weight matrix that stores the edge weights\n",
    "bh = tf.Variable(tf.zeros([1, n_hidden],  tf.float32, name=\"bh\")) #The bias vector for the hidden layer\n",
    "bv = tf.Variable(tf.zeros([1, n_visible],  tf.float32, name=\"bv\")) #The bias vector for the visible layer\n",
    "\n",
    "\n",
    "#### Helper functions. \n",
    "\n",
    "def sample(probs):\n",
    "    #Takes in a vector of probabilities, and returns a random vector of 0s and 1s sampled from the input vector\n",
    "    return tf.floor(probs + tf.random_uniform(tf.shape(probs), 0, 1))\n",
    "\n",
    "#This function runs the gibbs chain. We will call this function in two places:\n",
    "#    - When we define the training update step\n",
    "#    - When we sample our music segments from the trained RBM\n",
    "def gibbs_sample(k):\n",
    "    #Runs a k-step gibbs chain to sample from the probability distribution of the RBM defined by W, bh, bv\n",
    "    def gibbs_step(count, k, xk):\n",
    "        #Runs a single gibbs step. The visible values are initialized to xk\n",
    "        hk = sample(tf.sigmoid(tf.matmul(xk, W) + bh)) #Propagate the visible values to sample the hidden values\n",
    "        xk = sample(tf.sigmoid(tf.matmul(hk, tf.transpose(W)) + bv)) #Propagate the hidden values to sample the visible values\n",
    "        return count+1, k, xk\n",
    "\n",
    "    #Run gibbs steps for k iterations\n",
    "    ct = tf.constant(0) #counter\n",
    "    [_, _, x_sample] = control_flow_ops.while_loop(lambda count, num_iter, *args: count < num_iter,\n",
    "                                         gibbs_step, [ct, tf.constant(k), x])\n",
    "    #This is not strictly necessary in this implementation, but if you want to adapt this code to use one of TensorFlow's\n",
    "    #optimizers, you need this in order to stop tensorflow from propagating gradients back through the gibbs step\n",
    "    x_sample = tf.stop_gradient(x_sample) \n",
    "    return x_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Training Update Code\n",
    "# Now we implement the contrastive divergence algorithm. First, we get the samples of x and h from the probability distribution\n",
    "#The sample of x\n",
    "x_sample = gibbs_sample(1) \n",
    "#The sample of the hidden nodes, starting from the visible state of x\n",
    "h = sample(tf.sigmoid(tf.matmul(x, W) + bh)) \n",
    "#The sample of the hidden nodes, starting from the visible state of x_sample\n",
    "h_sample = sample(tf.sigmoid(tf.matmul(x_sample, W) + bh)) \n",
    "\n",
    "#Next, we update the values of W, bh, and bv, based on the difference between the samples that we drew and the original values\n",
    "size_bt = tf.cast(tf.shape(x)[0], tf.float32)\n",
    "W_adder  = tf.mul(lr/size_bt, tf.sub(tf.matmul(tf.transpose(x), h), tf.matmul(tf.transpose(x_sample), h_sample)))\n",
    "bv_adder = tf.mul(lr/size_bt, tf.reduce_sum(tf.sub(x, x_sample), 0, True))\n",
    "bh_adder = tf.mul(lr/size_bt, tf.reduce_sum(tf.sub(h, h_sample), 0, True))\n",
    "#When we do sess.run(updt), TensorFlow will run all 3 update steps\n",
    "updt = [W.assign_add(W_adder), bv.assign_add(bv_adder), bh.assign_add(bh_adder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Run the graph\n",
    "# Now it's time to start a session and run the graph! \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #First, we train the model\n",
    "    #initialize the variables of the model\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    #Run through all of the training data num_epochs times\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for song in songs:\n",
    "            #The songs are stored in a time x notes format. The size of each song is timesteps_in_song x 2*note_range\n",
    "            #Here we reshape the songs so that each training example is a vector with num_timesteps x 2*note_range elements\n",
    "            song = np.array(song)\n",
    "            song = song[:np.floor(song.shape[0]/num_timesteps)*num_timesteps]\n",
    "            song = np.reshape(song, [song.shape[0]/num_timesteps, song.shape[1]*num_timesteps])\n",
    "            #Train the RBM on batch_size examples at a time\n",
    "            for i in range(1, len(song), batch_size): \n",
    "                tr_x = song[i:i+batch_size]\n",
    "                sess.run(updt, feed_dict={x: tr_x})\n",
    "\n",
    "    #Now the model is fully trained, so let's make some music! \n",
    "    #Run a gibbs chain where the visible nodes are initialized to 0\n",
    "    sample = gibbs_sample(1).eval(session=sess, feed_dict={x: np.zeros((10, n_visible))})\n",
    "    for i in range(sample.shape[0]):\n",
    "        if not any(sample[i,:]):\n",
    "            continue\n",
    "        #Here we reshape the vector to be time x notes, and then save the vector as a midi file\n",
    "        S = np.reshape(sample[i,:], (num_timesteps, 2*note_range))\n",
    "        midi_manipulation.noteStateMatrixToMidi(S, \"generated_chord_{}\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example : AI Beating Pong .\n",
    "A simple example of an AI that beats Pong! Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym.\n",
    "\n",
    "Forwared and Backwared propagation are not implemented by me. The other parts are all implemented using numpy no Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle as pickle \n",
    "#OpenAI's library that provides environments to test RL algorithms in, Universe adds\n",
    "#even more environments\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4 #for convergence (too low- slow to converge, too high,never converge)\n",
    "gamma = 0.99 # discount factor for reward (i.e later rewards are exponentially less important)\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model initialization\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid (the pong world)\n",
    "if resume:\n",
    "    model = pickle.load(open('save.p', 'rb')) #load from pickled checkpoint\n",
    "else:\n",
    "    model = {} #initialize model \n",
    "    #rand returns a sample (or samples) from the standard normal distribution\n",
    "    #xavier algo determines the scale of initialization based on the number of input and output neurons.\n",
    "    #Imagine that your weights are initially very close to 0. What happens is that the signals shrink as it goes through each \n",
    "    #layer until it becomes too tiny to be useful. Now if your weights are too big, the signals grow at each layer \n",
    "    #it passes through until it is too massive to be useful.\n",
    "    #By using Xavier initialization, we make sure that the weights are not too small but not too big to propagate accurately the signals.\n",
    "    model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "    model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "    #zeros like returns an array of zeros with the same shape and type as a given array.\n",
    "    #we will update buffers that add up gradients over a batch\n",
    "    #where the model contains kv pairs, weights layers etc \n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.iteritems() } \n",
    "## rmsprop (gradient descent) memory used to update model\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.iteritems() } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#activation function\n",
    "def sigmoid(x): \n",
    "    return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "#takes a single game frame as input\n",
    "#preprocesses before feeding into model\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel() #flattens \n",
    "\n",
    "#since we are otimizing for short term reward, not long term reward (like legend of zelda)\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    #initilize discount reward matrix as empty\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    #to store reward sums\n",
    "    running_add = 0\n",
    "    #for each reward\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "    #if reward at index t is nonzero, reset the sum, since this was a game boundary (pong specific!)\n",
    "    if r[t] != 0: running_add = 0 \n",
    "    #increment the sum \n",
    "    #https://github.com/hunkim/ReinforcementZeroToAll/issues/1\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    #earlier rewards given more value over time \n",
    "    #assign the calculated sum to our discounted reward matrix\n",
    "    discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "\n",
    "#forward propagation via numpy\n",
    "def policy_forward(x):\n",
    "    #matrix multiply input by the first set of weights to get hidden state\n",
    "    #will be able to detect various game scenarios (e.g. the ball is in the top, and our paddle is in the middle)\n",
    "    h = np.dot(model['W1'], x)\n",
    "    #apply an activation function to it\n",
    "    #f(x)=max(0,x) take max value, if less than 0, use 0\n",
    "    h[h<0] = 0 # ReLU nonlinearity\n",
    "    #repeat process once more\n",
    "    #will decide if in each case we should be going UP or DOWN.\n",
    "    logp = np.dot(model['W2'], h)\n",
    "    #squash it with an activation (this time sigmoid to output probabilities)\n",
    "    p = sigmoid(logp)\n",
    "    return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "    \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "    #recursively compute error derivatives for both layers, this is the chain rule\n",
    "    #epdlopgp modulates the gradient with advantage\n",
    "    #compute updated derivative with respect to weight 2. It's the parameter hidden states transpose * gradient w/ advantage (then flatten with ravel())\n",
    "    dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "    #Compute derivative hidden. It's the outer product of gradient w/ advatange and weight matrix 2 of 2\n",
    "    dh = np.outer(epdlogp, model['W2'])\n",
    "    #apply activation\n",
    "    dh[eph <= 0] = 0 # backpro prelu\n",
    "    #compute derivative with respect to weight 1 using hidden states transpose and input observation\n",
    "    dW1 = np.dot(dh.T, epx)\n",
    "    #return both derivatives to update weights\n",
    "    return {'W1':dW1, 'W2':dW2}\n",
    "\n",
    "#environment\n",
    "env = gym.make(\"Pong-v0\")\n",
    "#Each timestep, the agent chooses an action, and the environment returns an observation and a reward.\n",
    "#The process gets started by calling reset, which returns an initial observation\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "#observation, hidden state, gradient, reward\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "#current reward\n",
    "running_reward = None\n",
    "#sum rewards\n",
    "reward_sum = 0\n",
    "#where are we?\n",
    "episode_number = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#begin training!\n",
    "while True:\n",
    "    \n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    prev_x = cur_x\n",
    "    #so x is our image difference, feed it in!\n",
    "\n",
    "    # forward the policy network and sample an action from the returned probability\n",
    "    aprob, h = policy_forward(x)\n",
    "    #this is the stochastic part \n",
    "    #since not apart of the model, model is easily differentiable\n",
    "    #if it was apart of the model, we'd have to use a reparametrization trick (a la variational autoencoders. so badass)\n",
    "    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n",
    "\n",
    "    # record various intermediates (needed later for backprop)\n",
    "    xs.append(x) # observation\n",
    "    hs.append(h) # hidden state\n",
    "    y = 1 if action == 2 else 0 # a \"fake label\"\n",
    "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
    "\n",
    "    # step the environment and get new measurements\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "\n",
    "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "    if done: # an episode finished\n",
    "    episode_number += 1\n",
    "\n",
    "    # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "    #each episode is a few dozen games\n",
    "    epx = np.vstack(xs) #obsveration\n",
    "    eph = np.vstack(hs) #hidden\n",
    "    epdlogp = np.vstack(dlogps) #gradient\n",
    "    epr = np.vstack(drs) #reward\n",
    "    xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "\n",
    "    #the strength with which we encourage a sampled action is the weighted sum of all rewards afterwards, but later rewards are exponentially less important\n",
    "    # compute the discounted reward backwards through time\n",
    "    discounted_epr = discount_rewards(epr)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "    discounted_epr -= np.mean(discounted_epr)\n",
    "    discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "    #advatnage - quantity which describes how good the action is compared to the average of all the action.\n",
    "    epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "    grad = policy_backward(eph, epdlogp)\n",
    "    for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "    # perform rmsprop parameter update every batch_size episodes\n",
    "    #http://68.media.tumblr.com/2d50e380d8e943afdfd66554d70a84a1/tumblr_inline_o4gfjnL2xK1toi3ym_500.png\n",
    "    if episode_number % batch_size == 0:\n",
    "        for k,v in model.iteritems():\n",
    "            g = grad_buffer[k] # gradient\n",
    "            rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "            model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "            grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "    # boring book-keeping\n",
    "    running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "    print('resetting env. episode reward total was %f. running mean: %f' % (reward_sum, running_reward))\n",
    "    if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
    "    reward_sum = 0\n",
    "    observation = env.reset() # reset env\n",
    "    prev_x = None\n",
    "\n",
    "    if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n",
    "    print ('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
