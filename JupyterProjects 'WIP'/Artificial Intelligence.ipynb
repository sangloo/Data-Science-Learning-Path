{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projects\n",
    "\n",
    "Deep Reiforcement learning is the cunning Artificial intelligence algorithm that exists nowdays, it is thanks to that kind of algorithms that Deep Mind won the Go Shampionship  this algorithms are behind every general purpose AI and evolutionary AI.\n",
    "\n",
    "These 4 projects go through the most important algorithms in this field i won't be exmplaining every one of them, check the machine learning and deep learning cheat sheets to get a better understanding, i'll try to comment as much lines of code as i can, and here are the first 4 projects i'll toggle, more will be coming later\n",
    "\n",
    "- Self Drviving car AI Deep Q-Learning\n",
    "- Teaching AI how to play DOOM Deep Convolutional Q learning\n",
    "- Atari games A3C\n",
    "- Udacity open source self driving car simulator AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "#### What is reiforcement Learning\n",
    "Unlike Machine and Deep learning algorithms, Reiforcement Learning is an always updating algorithm, our agent will receive reward to update and learn from mistakes...., the same process will go over and over again to further improve the agent, so the data needed to train in the model is made by the model itself accoring to the task need, and data is always feeding into the model as long as it is alive.\n",
    "\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/RL.JPG?raw=true)\n",
    "\n",
    "##### The bellman Equation\n",
    "- s: State\n",
    "- a: Action\n",
    "- R: Reward\n",
    "- Y: Discount factor\n",
    "\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/bellman.JPG?raw=true)\n",
    "\n",
    "We give directions to our agent, and give him rewards if reach goal, then using the bellman equation the computer generates some values of being of each state to get the best results... so in the end we don't program all the program but just some basic elements and we let the computer do the rest.\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/bll.JPG?raw=true)\n",
    "\n",
    "and based on these values the computer generates a plan\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/plan.JPG?raw=true)\n",
    "\n",
    "#### Markov Decision Process (MDP)\n",
    "But that of course is not all. as tochastic process has markov property if the conditional probability distribution of future states of the process(conditional on both past and present states) depends only upon the present state, not the sequence of events that preceded it, A process with this property is called a Markov process < == > When the future state will only depend on where you are now.\n",
    "\n",
    "\n",
    "### Deep Q-Learning: Self Driving Car\n",
    "\n",
    "(https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)\n",
    "Deep Q-Learning is the combination of a Q-Learning Algorithm with an Artificial Neural Network. here we'll be using Pytorch for the DQN implementation and Kivy for a basic simulation environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the AI\n",
    "#importing the libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the architecture of the NN\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, nb_action):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.nb_action = nb_action\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 30)\n",
    "        self.fc2 = nn.Linear(30, nb_action)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        q_values = self.fc2(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementing Experience Replay\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "    \n",
    "    def push(self, event):\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        samples = zip(*random.sample(self.memory, batch_size))\n",
    "        return map(lambda x: Variable(torch.cat(x, 0)), samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementing Deep Q Learning\n",
    "\n",
    "class Dqn():\n",
    "    \n",
    "    def __init__(self, input_size, nb_action, gamma):\n",
    "        self.gamma = gamma\n",
    "        self.reward_window = []\n",
    "        self.model = Network(input_size, nb_action)\n",
    "        self.memory = ReplayMemory(100000)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = 0.001)\n",
    "        self.last_state = torch.Tensor(input_size).unsqueeze(0)\n",
    "        self.last_action = 0\n",
    "        self.last_reward = 0\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        probs = F.softmax(self.model(Variable(state, volatile = True))*100) # T=100\n",
    "        action = probs.multinomial()\n",
    "        return action.data[0,0]\n",
    "    \n",
    "    def learn(self, batch_state, batch_next_state, batch_reward, batch_action):\n",
    "        outputs = self.model(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)\n",
    "        next_outputs = self.model(batch_next_state).detach().max(1)[0]\n",
    "        target = self.gamma*next_outputs + batch_reward\n",
    "        td_loss = F.smooth_l1_loss(outputs, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        td_loss.backward(retain_variables = True)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def update(self, reward, new_signal):\n",
    "        new_state = torch.Tensor(new_signal).float().unsqueeze(0)\n",
    "        self.memory.push((self.last_state, new_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([self.last_reward])))\n",
    "        action = self.select_action(new_state)\n",
    "        if len(self.memory.memory) > 100:\n",
    "            batch_state, batch_next_state, batch_action, batch_reward = self.memory.sample(100)\n",
    "            self.learn(batch_state, batch_next_state, batch_reward, batch_action)\n",
    "        self.last_action = action\n",
    "        self.last_state = new_state\n",
    "        self.last_reward = reward\n",
    "        self.reward_window.append(reward)\n",
    "        if len(self.reward_window) > 1000:\n",
    "            del self.reward_window[0]\n",
    "        return action\n",
    "    \n",
    "    def score(self):\n",
    "        return sum(self.reward_window)/(len(self.reward_window)+1.)\n",
    "    \n",
    "    def save(self):\n",
    "        torch.save({'state_dict': self.model.state_dict(),\n",
    "                    'optimizer' : self.optimizer.state_dict(),\n",
    "                   }, 'last_brain.pth')\n",
    "    \n",
    "    def load(self):\n",
    "        if os.path.isfile('last_brain.pth'):\n",
    "            print(\"=> loading checkpoint... \")\n",
    "            checkpoint = torch.load('last_brain.pth')\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"done !\")\n",
    "        else:\n",
    "            print(\"no checkpoint found...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The map\n",
    "#Importing the libraries\n",
    "import numpy as np\n",
    "from random import random, randint\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from kivy.app import App\n",
    "from kivy.uix.widget import Widget\n",
    "from kivy.uix.button import Button\n",
    "from kivy.graphics import Color, Ellipse, Line\n",
    "from kivy.config import Config\n",
    "from kivy.properties import NumericProperty, ReferenceListProperty, ObjectProperty\n",
    "from kivy.vector import Vector\n",
    "from kivy.clock import Clock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Config.set('input', 'mouse', 'mouse, multitouch_on_demand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# last_x and last_y are used to keep the last point inmemory when we draw the sand one the map\n",
    "last_x = 0\n",
    "last_y = 0\n",
    "n_points = 0 #Total number of points in the last drawing\n",
    "length = 0 #the length of the last drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Getting our AI, which we call brain contains the DQN\n",
    "brain = Dqn(5, 3, 0.9) #init with 5 sensors, 3actions, gama = 0.9\n",
    "action2rotation = [0, 20, -20] # actions ==> 0: no rotation, 1: 20deg rotation, 2: -20deg rotation\n",
    "last_reward = 0 #the last reward\n",
    "scores = [] # init the mean score curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initializing the map\n",
    "first_update = True #trick to only update the map once\n",
    "\n",
    "def init():\n",
    "    global sand #sand is an array that has as many cells as our graphic interface has pixels, each cell contain 1 if there is sand, and 0 if not\n",
    "    global goal_x #x_cord of the goal the car have to reach\n",
    "    global goal_y #y_cord of the goal the car have to reach\n",
    "    sand = np.zeros((longueur, largeur)) #init the sand array with zeros\n",
    "    goal_x = 20\n",
    "    goal_y = largeur - 20\n",
    "    first_update = False #the map has been initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#init the last distance\n",
    "last_distance = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the car class\n",
    "\n",
    "class Car(Widget):\n",
    "    \n",
    "    angle = NumericProperty(0) #init the angle of the car\n",
    "    rotation = NumericProperty(0) #last rotation of the car\n",
    "    \n",
    "    velocity_x = NumericProperty(0) #x cord of the velocity vector\n",
    "    velocity_y = NumericProperty(0) #y cord of the velocity vector\n",
    "    velocity = ReferenceListProperty(velocity_x, velocity_y) #velocity vector\n",
    "    \n",
    "    sensor1_x = NumericProperty(0) #X cord of 1st sensor\n",
    "    sensor1_y = NumericProperty(0) #Y cord of 1st sensor\n",
    "    sensor1 = ReferenceListProperty(sensor1_x, sensor1_y) # first sensor vector\n",
    "    \n",
    "    sensor2_x = NumericProperty(0)\n",
    "    sensor2_y = NumericProperty(0)\n",
    "    sensor2 = ReferenceListProperty(sensor2_x, sensor2_y)\n",
    "    \n",
    "    sensor3_x = NumericProperty(0)\n",
    "    sensor3_y = NumericProperty(0)\n",
    "    sensor3 = ReferenceListProperty(sensor3_x, sensor3_y)\n",
    "    \n",
    "    signal1 = NumericProperty(0) #init the signal received by sensor 1\n",
    "    signal2 = NumericProperty(0) #init the signal received by sensor 2\n",
    "    signal3 = NumericProperty(0) #init the signal received by sensor 3\n",
    "    \n",
    "    def move(self, rotation):\n",
    "        self.pos = Vector(*self.velocity) + self.pos # updating the position of the car according to its last position and velocity\n",
    "        self.rotation = rotation # getting the rotation of the car\n",
    "        self.angle = self.angle + self.rotation # updating the angle\n",
    "        \n",
    "        self.sensor1 = Vector(30, 0).rotate(self.angle) + self.pos # updating the position of sensor 1\n",
    "        self.sensor2 = Vector(30, 0).rotate((self.angle+30)%360) + self.pos # updating the position of sensor 2\n",
    "        self.sensor3 = Vector(30, 0).rotate((self.angle-30)%360) + self.pos # updating the position of sensor 3\n",
    "        \n",
    "        self.signal1 = int(np.sum(sand[int(self.sensor1_x)-10:int(self.sensor1_x)+10, int(self.sensor1_y)-10:int(self.sensor1_y)+10]))/400. # getting the signal received by sensor 1 (density of sand around sensor 1)\n",
    "        self.signal2 = int(np.sum(sand[int(self.sensor2_x)-10:int(self.sensor2_x)+10, int(self.sensor2_y)-10:int(self.sensor2_y)+10]))/400. # getting the signal received by sensor 2 (density of sand around sensor 2)\n",
    "        self.signal3 = int(np.sum(sand[int(self.sensor3_x)-10:int(self.sensor3_x)+10, int(self.sensor3_y)-10:int(self.sensor3_y)+10]))/400. # getting the signal received by sensor 3 (density of sand around sensor 3)\n",
    "        \n",
    "        if self.sensor1_x > longueur-10 or self.sensor1_x<10 or self.sensor1_y>largeur-10 or self.sensor1_y<10: # if sensor 1 is out of the map (the car is facing one edge of the map)\n",
    "            self.signal1 = 1. # sensor 1 detects full sand\n",
    "        if self.sensor2_x > longueur-10 or self.sensor2_x<10 or self.sensor2_y>largeur-10 or self.sensor2_y<10: # if sensor 2 is out of the map (the car is facing one edge of the map)\n",
    "            self.signal2 = 1. # sensor 2 detects full sand\n",
    "        if self.sensor3_x > longueur-10 or self.sensor3_x<10 or self.sensor3_y>largeur-10 or self.sensor3_y<10: # if sensor 3 is out of the map (the car is facing one edge of the map)\n",
    "            self.signal3 = 1. # sensor 3 detects full sand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Ball1(Widget): # sensor 1 (see kivy tutorials: kivy https://kivy.org/docs/tutorials/pong.html)\n",
    "    pass\n",
    "class Ball2(Widget): # sensor 2 (see kivy tutorials: kivy https://kivy.org/docs/tutorials/pong.html)\n",
    "    pass\n",
    "class Ball3(Widget): # sensor 3 (see kivy tutorials: kivy https://kivy.org/docs/tutorials/pong.html)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the game class (to understand \"ObjectProperty\", see kivy tutorials: kivy https://kivy.org/docs/tutorials/pong.html)\n",
    "\n",
    "class Game(Widget):\n",
    "\n",
    "    car = ObjectProperty(None) # getting the car object from our kivy file\n",
    "    \n",
    "    ball1 = ObjectProperty(None) # getting the sensor 1 object from our kivy file\n",
    "    ball2 = ObjectProperty(None) # getting the sensor 2 object from our kivy file\n",
    "    ball3 = ObjectProperty(None) # getting the sensor 3 object from our kivy file\n",
    "\n",
    "    def serve_car(self): # starting the car when we launch the application\n",
    "        self.car.center = self.center # the car will start at the center of the map\n",
    "        self.car.velocity = Vector(6, 0) # the car will start to go horizontally to the right with a speed of 6\n",
    "\n",
    "    def update(self, dt): # the big update function that updates everything that needs to be updated at each discrete time t when reaching a new state (getting new signals from the sensors)\n",
    "\n",
    "        global brain # specifying the global variables (the brain of the car, that is our AI)\n",
    "        global last_reward # specifying the global variables (the last reward)\n",
    "        global scores # specifying the global variables (the means of the rewards)\n",
    "        global last_distance # specifying the global variables (the last distance from the car to the goal)\n",
    "        global goal_x # specifying the global variables (x-coordinate of the goal)\n",
    "        global goal_y # specifying the global variables (y-coordinate of the goal)\n",
    "        global longueur # specifying the global variables (width of the map)\n",
    "        global largeur # specifying the global variables (height of the map)\n",
    "\n",
    "        longueur = self.width # width of the map (horizontal edge)\n",
    "        largeur = self.height # height of the map (vertical edge)\n",
    "        \n",
    "        if first_update: # trick to initialize the map only once\n",
    "            init()\n",
    "\n",
    "        xx = goal_x - self.car.x # difference of x-coordinates between the goal and the car\n",
    "        yy = goal_y - self.car.y # difference of y-coordinates between the goal and the car\n",
    "        \n",
    "        orientation = Vector(*self.car.velocity).angle((xx,yy))/180. # direction of the car with respect to the goal (if the car is heading perfectly towards the goal, then orientation = 0)\n",
    "        \n",
    "        last_signal = [self.car.signal1, self.car.signal2, self.car.signal3, orientation, -orientation] # our input state vector, composed of the three signals received by the three sensors, plus the orientation and -orientation\n",
    "        \n",
    "        action = brain.update(last_reward, last_signal) # playing the action from our ai (the object brain of the dqn class)\n",
    "        \n",
    "        scores.append(brain.score()) # appending the score (mean of the last 100 rewards to the reward window)\n",
    "        \n",
    "        rotation = action2rotation[action] # converting the action played (0, 1 or 2) into the rotation angle (0°, 20° or -20°)\n",
    "        \n",
    "        self.car.move(rotation) # moving the car according to this last rotation angle\n",
    "        \n",
    "        distance = np.sqrt((self.car.x - goal_x)**2 + (self.car.y - goal_y)**2) # getting the new distance between the car and the goal right after the car moved\n",
    "        \n",
    "        self.ball1.pos = self.car.sensor1 # updating the position of the first sensor (ball1) right after the car moved\n",
    "        self.ball2.pos = self.car.sensor2 # updating the position of the second sensor (ball2) right after the car moved\n",
    "        self.ball3.pos = self.car.sensor3 # updating the position of the third sensor (ball3) right after the car moved\n",
    "\n",
    "        if sand[int(self.car.x),int(self.car.y)] > 0: # if the car is on the sand\n",
    "            self.car.velocity = Vector(1, 0).rotate(self.car.angle) # it is slowed down (speed = 1)\n",
    "            last_reward = -1 # and reward = -1\n",
    "        else: # otherwise\n",
    "            self.car.velocity = Vector(6, 0).rotate(self.car.angle) # it goes to a normal speed (speed = 6)\n",
    "            last_reward = -0.2 # and it gets bad reward (-0.2)\n",
    "            if distance < last_distance: # however if it getting close to the goal\n",
    "                last_reward = 0.1 # it still gets slightly positive reward 0.1\n",
    "\n",
    "        if self.car.x < 10: # if the car is in the left edge of the frame\n",
    "            self.car.x = 10 # it is not slowed down\n",
    "            last_reward = -1 # but it gets bad reward -1\n",
    "            \n",
    "        if self.car.x > self.width-10: # if the car is in the right edge of the frame\n",
    "            self.car.x = self.width-10 # it is not slowed down\n",
    "            last_reward = -1 # but it gets bad reward -1\n",
    "            \n",
    "        if self.car.y < 10: # if the car is in the bottom edge of the frame\n",
    "            self.car.y = 10 # it is not slowed down\n",
    "            last_reward = -1 # but it gets bad reward -1\n",
    "            \n",
    "        if self.car.y > self.height-10: # if the car is in the upper edge of the frame\n",
    "            self.car.y = self.height-10 # it is not slowed down\n",
    "            last_reward = -1 # but it gets bad reward -1\n",
    "\n",
    "        if distance < 100: # when the car reaches its goal\n",
    "            goal_x = self.width - goal_x # the goal becomes the bottom right corner of the map (the downtown), and vice versa (updating of the x-coordinate of the goal)\n",
    "            goal_y = self.height - goal_y # the goal becomes the bottom right corner of the map (the downtown), and vice versa (updating of the y-coordinate of the goal)\n",
    "\n",
    "        # Updating the last distance from the car to the goal\n",
    "        last_distance = distance\n",
    "\n",
    "# Painting for graphic interface (see kivy tutorials: https://kivy.org/docs/tutorials/firstwidget.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyPaintWidget(Widget):\n",
    "\n",
    "    def on_touch_down(self, touch): # putting some sand when we do a left click\n",
    "        global length,n_points,last_x,last_y\n",
    "        \n",
    "        with self.canvas:\n",
    "            Color(0.8,0.7,0)\n",
    "            d=10.\n",
    "            touch.ud['line'] = Line(points = (touch.x, touch.y), width = 10)\n",
    "            last_x = int(touch.x)\n",
    "            last_y = int(touch.y)\n",
    "            n_points = 0\n",
    "            length = 0\n",
    "            sand[int(touch.x),int(touch.y)] = 1\n",
    "\n",
    "    def on_touch_move(self, touch): # putting some sand when we move the mouse while pressing left\n",
    "        global length,n_points,last_x,last_y\n",
    "        \n",
    "        if touch.button=='left':\n",
    "            touch.ud['line'].points += [touch.x, touch.y]\n",
    "            x = int(touch.x)\n",
    "            y = int(touch.y)\n",
    "            \n",
    "            length += np.sqrt(max((x - last_x)**2 + (y - last_y)**2, 2))\n",
    "            n_points += 1.\n",
    "            density = n_points/(length)\n",
    "            touch.ud['line'].width = int(20*density + 1)\n",
    "            sand[int(touch.x) - 10 : int(touch.x) + 10, int(touch.y) - 10 : int(touch.y) + 10] = 1\n",
    "            \n",
    "            last_x = x\n",
    "            last_y = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CarApp(App):\n",
    "\n",
    "    def build(self): # building the app\n",
    "        parent = Game()\n",
    "        parent.serve_car()\n",
    "        Clock.schedule_interval(parent.update, 1.0 / 60.0)\n",
    "        self.painter = MyPaintWidget()\n",
    "        clearbtn = Button(text='clear')\n",
    "        savebtn = Button(text='save',pos=(parent.width,0))\n",
    "        loadbtn = Button(text='load',pos=(2*parent.width,0))\n",
    "        clearbtn.bind(on_release=self.clear_canvas)\n",
    "        savebtn.bind(on_release=self.save)\n",
    "        loadbtn.bind(on_release=self.load)\n",
    "        parent.add_widget(self.painter)\n",
    "        parent.add_widget(clearbtn)\n",
    "        parent.add_widget(savebtn)\n",
    "        parent.add_widget(loadbtn)\n",
    "        return parent\n",
    "\n",
    "    def clear_canvas(self, obj): # clear button\n",
    "        global sand\n",
    "        self.painter.canvas.clear()\n",
    "        sand = np.zeros((longueur,largeur))\n",
    "\n",
    "    def save(self, obj): # save button\n",
    "        print(\"saving brain...\")\n",
    "        brain.save()\n",
    "        plt.plot(scores)\n",
    "        plt.show()\n",
    "\n",
    "    def load(self, obj): # load button\n",
    "        print(\"loading last saved brain...\")\n",
    "        brain.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CarApp().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Doom openai gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "from scipy.misc import imresize\n",
    "from gym.core import ObservationWrapper\n",
    "from gym.spaces.box import Box\n",
    "\n",
    "# Preprocessing the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PreprocessImage(ObservationWrapper):\n",
    "    \n",
    "    def __init__(self, env, height = 64, width = 64, grayscale = True, crop = lambda img: img):\n",
    "        super(PreprocessImage, self).__init__(env)\n",
    "        self.img_size = (height, width)\n",
    "        self.grayscale = grayscale\n",
    "        self.crop = crop\n",
    "        n_colors = 1 if self.grayscale else 3\n",
    "        self.observation_space = Box(0.0, 1.0, [n_colors, height, width])\n",
    "\n",
    "    def _observation(self, img):\n",
    "        img = self.crop(img)\n",
    "        img = imresize(img, self.img_size)\n",
    "        if self.grayscale:\n",
    "            img = img.mean(-1, keepdims = True)\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "        img = img.astype('float32') / 255.\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Experience Replay\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining one Step\n",
    "Step = namedtuple('Step', ['state', 'action', 'reward', 'done'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NStepProgress:\n",
    "    \n",
    "    def __init__(self, env, ai, n_step):\n",
    "        self.ai = ai\n",
    "        self.rewards = []\n",
    "        self.env = env\n",
    "        self.n_step = n_step\n",
    "    \n",
    "    def __iter__(self):\n",
    "        state = self.env.reset()\n",
    "        history = deque()\n",
    "        reward = 0.0\n",
    "        while True:\n",
    "            action = self.ai(np.array([state]))[0][0]\n",
    "            next_state, r, is_done, _ = self.env.step(action)\n",
    "            reward += r\n",
    "            history.append(Step(state = state, action = action, reward = r, done = is_done))\n",
    "            while len(history) > self.n_step + 1:\n",
    "                history.popleft()\n",
    "            if len(history) == self.n_step + 1:\n",
    "                yield tuple(history)\n",
    "            state = next_state\n",
    "            if is_done:\n",
    "                if len(history) > self.n_step + 1:\n",
    "                    history.popleft()\n",
    "                while len(history) >= 1:\n",
    "                    yield tuple(history)\n",
    "                    history.popleft()\n",
    "                self.rewards.append(reward)\n",
    "                reward = 0.0\n",
    "                state = self.env.reset()\n",
    "                history.clear()\n",
    "    \n",
    "    def rewards_steps(self):\n",
    "        rewards_steps = self.rewards\n",
    "        self.rewards = []\n",
    "        return rewards_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementing Experience Replay\n",
    "\n",
    "class ReplayMemory:\n",
    "    \n",
    "    def __init__(self, n_steps, capacity = 10000):\n",
    "        self.capacity = capacity\n",
    "        self.n_steps = n_steps\n",
    "        self.n_steps_iter = iter(n_steps)\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def sample_batch(self, batch_size): # creates an iterator that returns random batches\n",
    "        ofs = 0\n",
    "        vals = list(self.buffer)\n",
    "        np.random.shuffle(vals)\n",
    "        while (ofs+1)*batch_size <= len(self.buffer):\n",
    "            yield vals[ofs*batch_size:(ofs+1)*batch_size]\n",
    "            ofs += 1\n",
    "\n",
    "    def run_steps(self, samples):\n",
    "        while samples > 0:\n",
    "            entry = next(self.n_steps_iter) # 10 consecutive steps\n",
    "            self.buffer.append(entry) # we put 200 for the current episode\n",
    "            samples -= 1\n",
    "        while len(self.buffer) > self.capacity: # we accumulate no more than the capacity (10000)\n",
    "            self.buffer.popleft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AI for Doom\n",
    "\n",
    "\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing the packages for OpenAI and Doom\n",
    "import gym\n",
    "from gym.wrappers import SkipWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the brain\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, number_actions):\n",
    "        super(CNN, self).__init__()\n",
    "        self.convolution1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 5)\n",
    "        self.convolution2 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3)\n",
    "        self.convolution3 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 2)\n",
    "        self.fc1 = nn.Linear(in_features = self.count_neurons((1, 80, 80)), out_features = 40)\n",
    "        self.fc2 = nn.Linear(in_features = 40, out_features = number_actions)\n",
    "\n",
    "    def count_neurons(self, image_dim):\n",
    "        x = Variable(torch.rand(1, *image_dim))\n",
    "        x = F.relu(F.max_pool2d(self.convolution1(x), 3, 2))\n",
    "        x = F.relu(F.max_pool2d(self.convolution2(x), 3, 2))\n",
    "        x = F.relu(F.max_pool2d(self.convolution3(x), 3, 2))\n",
    "        return x.data.view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.convolution1(x), 3, 2))\n",
    "        x = F.relu(F.max_pool2d(self.convolution2(x), 3, 2))\n",
    "        x = F.relu(F.max_pool2d(self.convolution3(x), 3, 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making the body\n",
    "\n",
    "class SoftmaxBody(nn.Module):\n",
    "    \n",
    "    def __init__(self, T):\n",
    "        super(SoftmaxBody, self).__init__()\n",
    "        self.T = T\n",
    "\n",
    "    def forward(self, outputs):\n",
    "        probs = F.softmax(outputs * self.T)   \n",
    "        actions = probs.multinomial()\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making the AI\n",
    "\n",
    "class AI:\n",
    "\n",
    "    def __init__(self, brain, body):\n",
    "        self.brain = brain\n",
    "        self.body = body\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        input = Variable(torch.from_numpy(np.array(inputs, dtype = np.float32)))\n",
    "        output = self.brain(input)\n",
    "        actions = self.body(output)\n",
    "        return actions.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 - Training the AI with Deep Convolutional Q-Learning\n",
    "\n",
    "# Getting the Doom environment\n",
    "doom_env = image_preprocessing.PreprocessImage(SkipWrapper(4)(ToDiscrete(\"minimal\")(gym.make(\"ppaquette/DoomCorridor-v0\"))), width = 80, height = 80, grayscale = True)\n",
    "doom_env = gym.wrappers.Monitor(doom_env, \"videos\", force = True)\n",
    "number_actions = doom_env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building an AI\n",
    "cnn = CNN(number_actions)\n",
    "softmax_body = SoftmaxBody(T = 1.0)\n",
    "ai = AI(brain = cnn, body = softmax_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting up Experience Replay\n",
    "n_steps = experience_replay.NStepProgress(env = doom_env, ai = ai, n_step = 10)\n",
    "memory = experience_replay.ReplayMemory(n_steps = n_steps, capacity = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementing Eligibility Trace\n",
    "def eligibility_trace(batch):\n",
    "    gamma = 0.99\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for series in batch:\n",
    "        input = Variable(torch.from_numpy(np.array([series[0].state, series[-1].state], dtype = np.float32)))\n",
    "        output = cnn(input)\n",
    "        cumul_reward = 0.0 if series[-1].done else output[1].data.max()\n",
    "        for step in reversed(series[:-1]):\n",
    "            cumul_reward = step.reward + gamma * cumul_reward\n",
    "        state = series[0].state\n",
    "        target = output[0].data\n",
    "        target[series[0].action] = cumul_reward\n",
    "        inputs.append(state)\n",
    "        targets.append(target)\n",
    "    return torch.from_numpy(np.array(inputs, dtype = np.float32)), torch.stack(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making the moving average on 100 steps\n",
    "class MA:\n",
    "    def __init__(self, size):\n",
    "        self.list_of_rewards = []\n",
    "        self.size = size\n",
    "    def add(self, rewards):\n",
    "        if isinstance(rewards, list):\n",
    "            self.list_of_rewards += rewards\n",
    "        else:\n",
    "            self.list_of_rewards.append(rewards)\n",
    "        while len(self.list_of_rewards) > self.size:\n",
    "            del self.list_of_rewards[0]\n",
    "    def average(self):\n",
    "        return np.mean(self.list_of_rewards)\n",
    "ma = MA(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training the AI\n",
    "loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr = 0.001)\n",
    "nb_epochs = 100\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    memory.run_steps(200)\n",
    "    for batch in memory.sample_batch(128):\n",
    "        inputs, targets = eligibility_trace(batch)\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        predictions = cnn(inputs)\n",
    "        loss_error = loss(predictions, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss_error.backward()\n",
    "        optimizer.step()\n",
    "    rewards_steps = n_steps.rewards_steps()\n",
    "    ma.add(rewards_steps)\n",
    "    avg_reward = ma.average()\n",
    "    print(\"Epoch: %s, Average Reward: %s\" % (str(epoch), str(avg_reward)))\n",
    "    if avg_reward >= 1500:\n",
    "        print(\"Congratulations, your AI wins\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Closing the Doom environment\n",
    "doom_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
