{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Artificial Neural Networks\n",
    "##### The Neuron\n",
    "The main purpose of Deep Learning is to mimic how Human Brain wroks. The Neuron are in the heart of the Human brain and are what give us the processing power, in a normal Human Brain there something like 100 Billion Neurons, and every one of these is connected to thousands of other Neurons. \n",
    "![](https://vignette1.wikia.nocookie.net/athletics/images/0/01/Neuron.gif/revision/latest?cb=20080929213518)\n",
    "We can separate a Neuron into three main parts\n",
    "1. The Neuron: A neuron by itself is pretty useless, and is only useful once connected to other Neurons\n",
    "2. Dendrites: The receiveires of the signals that come from other Neurons\n",
    "3. Axon: The senders of signals to other Neurons\n",
    "\n",
    "In Data Science A Neuron works a bit similarly, a Neuron is also called a Node, and one or multiple input signals or input value (x1, x2, ...., xn) and an or multiple output Signals\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png)\n",
    "But what does the input layer, the Neuron and the output layer contains??\n",
    "1. A simple way to think of input layers is in terms of observations,One input variables is one row of our data for example, Unlike some machine learning algorithms in Deep learning all these variables should be standardized/normalized. \n",
    "2. OUtput Value can be Continuous, Binary, or a Categorical Variable depending on what we want to ahcieve.\n",
    "3. The synapses are the connectors that connect each input layer to the Neuron, and these are crucial to neural network functionning because each of these contain a weight signal that decide how much a layer is important[W1, W2, ...., Wn]\n",
    "4. The Neuron: All the values from the synapses are added **\"Sum of Wi * Xi\"** and then it applies an acctivation function to this weighted sum. and that's the signal that the Neuron will either pass or not to the next Neuron down the line.\n",
    "\n",
    "But What is the activation Function?\n",
    "##### The Activation Function\n",
    "There is a lot of Activation Functions the predominant ones are\n",
    "![](http://www.yaldex.com/game-development/FILES/19fig02.gif)\n",
    "- The Threshold Function: if the value is less than 0 the function passes 0 and if the value is bigger that 0 then the answer is 1\n",
    "- Sigmoid Function = (1 / 1+ exp(-x)) the difference from the threshold function is that it is smoother.\n",
    "- Rectifier Function = max(x, 0): starts from zero and then linearly progress.\n",
    "-  Hyperbolic Tangent = (1 - exp(-2x)) / (1 + exp(-2x))  a bit like the Sigmoid function the only difference is that it goes beyond zero ...\n",
    "\n",
    "To Understand more (http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf)\n",
    "\n",
    "##### How do Neural Network Works?\n",
    "A very basic visual example for how a trained NN works for price estimation\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/TrainedNN.JPG?raw=true)\n",
    "\n",
    "This is an example of an already trained Neural Network, So one thing we notice here is that every Neuron is not connected to all uinput values, for example the last one is only connected to age, why? Maybe age is not important for deciding on a property price, but what if tthe property is over 100 years old or 200 years old, well that's what that neuron tells us, if the proprety is over 100 yo for instance then suddenly the age of it becomes the most significant price factor, because it is a historical building and the older is a historical building the higher the price, of course they may be other factors, but these other factors are taken care for in the other neurons, the first neurons only takes into consideration the area and distance to city...., and then an Activation function is applied to each of these neurons, it may be a different function for each one of the neurons, and once weighted together the output is show. How the weighting works, well like exaplained for the age neuron, if the age is 99 years old then this neuron will have a weight of maybe 0.05, while if it is a 250 yo building then this will become the main Neuron and will have a weight of maybe 0.5!! or even higher depending on how the Network has been trained.\n",
    "\n",
    "##### How do Neural Networks Learn?\n",
    "This is a one layer Neural Network also called a Perceptron\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/Perceptron.JPG?raw=true)\n",
    "So we have our three input values that will be supplied to the NN and then our Activation function will be applied and then will send an output.\n",
    "\n",
    "There is another factor that is called the Cost Function **(Y is the actual value and YY is the Output value)** a Cost function will compare the Y, and YY, **C = 1/2(YY - Y)²**, this is an example of a cost function. like with the Activation function there is a lot of cost functions, but basically what these functions tells us is the error that we have in our prediction, and our goal is to minimize the cost function. Once we have compared the output value goes back into the neural net and goes up to the weight to update it and correct it and go back to minize the cost function. \n",
    "\n",
    "This before is what happens in one row now to understand even better what happens when we feed multiple rows\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/MultiRows.JPG?raw=true)\n",
    "\n",
    "Here we're trying to train the model on the relationship between Study hours, sleep hours, quizz marks to predict the exam mark. It is o course the same NN applied to all of these rows, for every row we have a YY, then we compare to the Y of every row, the cost function here will be the **SUM of (C = 1/2(YY - Y)²)**, now that we have the full cost function, we go back and update the weights, the difference is that here the weights are the same for every single row, unlike the example before where the row had it's own weight values.\n",
    "\n",
    "##### Gradient Descent\n",
    "in Order for a NN to learn it needs backpropagation to adjust the weight. Here again is a simple example of a NN\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/GD.JPG?raw=true)\n",
    "A Question that might PopUp is how to adjust the weights? One way is The BRUT FORCE method where we try most of the possible values and take the optimal one. but this won't work if we have a thousand Input values and a lot of different Neurons, so a better way is **Gradient Descent**.\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/GradientDescent.JPG?raw=true)\n",
    "This image is a reprensentation of our problem, on the x axis we have the YY value, and in the y axis we have Cost function value. What the gradient descent does is that it starts somewhere (red dot) and calculate the slope, if the slope is positive then whe're going downhill right, then again, we calculate the slope if its negative we go left untill we find the best situation that minimize the cost function. this is what happens in the simplest possible way, of course in a real world problem, we should define the parameters of the GD function to optimize its behaviour to not go the wrong way for example....\n",
    "\n",
    "##### Stochastic Gradient Descent\n",
    "One problem with the gradient descent method is that it only works well with convex cost function, what if our cost function looks like this:\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/SGD.JPG?raw=true)\n",
    "What may happen in that case, is that we may fall into finding a local minimum and not the absolute minimum. What we seen earlier in our Neural Network training is a Batch Gradient Descent, we minimized the value taking into consideration all the rows in the same time. In this case the answer is **Stochastic Gradient Descent SGD**, in the SGD we take every row and minimize the cost function for that row, then go to the second row and minimize it....., and that allow us to avoid falling into that local minimum,  and while we may think it may be slower than the BGD the truth is that it's faster than the BGD,  more info \"https://iamtrask.github.io/2015/07/27/python-network-part2/\"\n",
    "\n",
    "\n",
    "##### Backpropagation\n",
    "Backpropagation like explained before, is the process is going up the Neural Network to optimize the weight values, Backpropagation adjust all the weights simultaneously. During this process because of the way the algorithm is structered, we can know which if the weights is responsible. \"http://neuralnetworksanddeeplearning.com/chap2.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Artificial Neural Network\n",
    "Churn Modeling: Understand why customers leave the Bank, so we are facing a binary classification problem.\n",
    "\n",
    "Libraries we can use for such a model are Theano, Tensorflow, and Keras. Keras is wrapper around Theano and Tensorflow makes it easier to build Deep Neural Networks...\n",
    "##### Part1: Data Preprocessing\n",
    "Like with machine Learning Data need to be Preprocessed before starting the training of the model in order to produce the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing our data\n",
    "data = pd.read_csv('data/Churn_Modelling.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 'France', 'Female', ..., 1, 1, 101348.88],\n",
       "       [608, 'Spain', 'Female', ..., 0, 1, 112542.58],\n",
       "       [502, 'France', 'Female', ..., 1, 0, 113931.57],\n",
       "       ..., \n",
       "       [709, 'France', 'Female', ..., 0, 1, 42085.58],\n",
       "       [772, 'Germany', 'Male', ..., 1, 0, 92888.52],\n",
       "       [792, 'France', 'Female', ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting our data into the Matrix of features and our dependant variable\n",
    "# Exied is our dependant variable, and the rest will become the matrix of features\n",
    "# There is lots of ways to solve this problem, this is the straighforward way\n",
    "# We will drop the rowNumber, customerId, and surname because they have no impact\n",
    "# on the decision of leaving the bank\n",
    "X = data.iloc[:, 3:13].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data.iloc[:, 13].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 0, 'Female', ..., 1, 1, 101348.88],\n",
       "       [608, 2, 'Female', ..., 0, 1, 112542.58],\n",
       "       [502, 0, 'Female', ..., 1, 0, 113931.57],\n",
       "       ..., \n",
       "       [709, 0, 'Female', ..., 0, 1, 42085.58],\n",
       "       [772, 1, 'Male', ..., 1, 0, 92888.52],\n",
       "       [792, 0, 'Female', ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we should encode our categorical variables the country and gender columns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([619, 0, 0, 42, 2, 0.0, 1, 1, 1, 101348.88], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         6.19000000e+02,   0.00000000e+00,   4.20000000e+01,\n",
       "         2.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "         1.00000000e+00,   1.00000000e+00,   1.01348880e+05])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.00000000e+00,   0.00000000e+00,   6.19000000e+02,\n",
       "         0.00000000e+00,   4.20000000e+01,   2.00000000e+00,\n",
       "         0.00000000e+00,   1.00000000e+00,   1.00000000e+00,\n",
       "         1.00000000e+00,   1.01348880e+05])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X[:, 1:]\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting the dataset into Training set and Test Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, \n",
    "                                                   random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5698444 ,  1.74309049,  0.16958176, -1.09168714, -0.46460796,\n",
       "        0.00666099, -1.21571749,  0.8095029 ,  0.64259497, -1.03227043,\n",
       "        1.10643166])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature scaling is compulsory in this case\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Now we will start making the Artificial Neural Network\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initializing our Neural Net\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A reminder of the steps to build the neural net:\n",
    "1. Randomly initialise the weights to small numbers close to 0 (not zero)\n",
    "2. Input the first observation of the dataset in the input layer, each feature in one input node\n",
    "3. Forward-Propagation: from left to right, the neurons are activated in a way that th impact of each neuron's activation is limited by the weights. Propagate the activations until getting the predicted result y.\n",
    "4. Compare the predicted result to the actual result. Measure the generated error\n",
    "5. Back Propagation: from right to left, the error is back-propagated, update the weights according to how much they are responsible for the error, the learning rate decides by how much we update the weights\n",
    "6. Repeat step 1 to 5 and update the weights after each observation (Reinforcement Learning) or repeating the first 5 steps and only update the weights after a batch of observations (Batch learning)\n",
    "7. When the whole training set passed through the ANN, that makes an epoch. Redo more epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding the input layer and the first hidden layer\n",
    "#Units is the number of nodes\n",
    "#kernel_initializer is the way the weights will be initialized\n",
    "#activation is the activation function\n",
    "#input dim is here our input layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu',\n",
    "                    input_dim = 11))\n",
    "classifier.add(Dropout(rate = 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding another hidden layer \"not necessary here\"\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform',\n",
    "                     activation = 'relu'))\n",
    "classifier.add(Dropout(rate = 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform',\n",
    "                     activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compiling the ANN \"applying the stochastic gradient descent\"\n",
    "#optimizer: the SGD algo here it's adam\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy',\n",
    "                   metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4917 - acc: 0.7954     - ETA: 1s - loss: 0.5639 - \n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4427 - acc: 0.7960     \n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4338 - acc: 0.7960     \n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4361 - acc: 0.7960     \n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4333 - acc: 0.7960     \n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4303 - acc: 0.7960     - ETA: 0s - loss: 0.4148 - \n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4310 - acc: 0.7960     \n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4307 - acc: 0.7960     \n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4324 - acc: 0.7999     \n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4319 - acc: 0.7974     \n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4295 - acc: 0.8171     \n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4330 - acc: 0.8195     \n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4307 - acc: 0.8236     \n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4341 - acc: 0.8220     \n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4324 - acc: 0.8231     \n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4308 - acc: 0.8254     \n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4313 - acc: 0.8251     \n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4314 - acc: 0.8267     \n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4276 - acc: 0.8237     \n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4285 - acc: 0.8270     \n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4270 - acc: 0.8279     \n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4293 - acc: 0.8265     \n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4315 - acc: 0.8255     - ETA: 0s - loss: 0.4258 - a\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4264 - acc: 0.8280     \n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4303 - acc: 0.8286     \n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4308 - acc: 0.8282     \n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4287 - acc: 0.8294     - ETA: 0s - loss: 0.4357 - acc: 0.8 - ETA: 0s - loss: 0.4366 - acc: 0\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4283 - acc: 0.8295     \n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4311 - acc: 0.8269     \n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4295 - acc: 0.8287     \n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4267 - acc: 0.8285     \n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4270 - acc: 0.8284     \n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4276 - acc: 0.8290     \n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4282 - acc: 0.8291     - ETA: 0s - loss: 0.4280 - acc: 0.82\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4292 - acc: 0.8289     \n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4282 - acc: 0.8301     \n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4281 - acc: 0.8286     - ETA: 0s - loss: 0.4265 - acc: 0.8\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4279 - acc: 0.8287     \n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4300 - acc: 0.8297     \n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4277 - acc: 0.8284     \n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4279 - acc: 0.8291     \n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4290 - acc: 0.8296     \n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4269 - acc: 0.8307     \n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4285 - acc: 0.8305     - ETA: 0s - loss: 0.4288 - acc: 0.830\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4297 - acc: 0.8310     \n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4289 - acc: 0.8295     \n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4248 - acc: 0.8296     \n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4271 - acc: 0.8314     \n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4250 - acc: 0.8299     \n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4275 - acc: 0.8284     \n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4307 - acc: 0.8321     \n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4299 - acc: 0.8300     \n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4269 - acc: 0.8307     \n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4277 - acc: 0.8296     \n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4301 - acc: 0.8290     \n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4287 - acc: 0.8315     \n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4269 - acc: 0.8301     \n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4280 - acc: 0.8319     \n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4289 - acc: 0.8300     \n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4247 - acc: 0.8292     \n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4276 - acc: 0.8311     \n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4247 - acc: 0.8280     \n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4281 - acc: 0.8300     \n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.4237 - acc: 0.8307     \n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4247 - acc: 0.8277     \n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4270 - acc: 0.8299     \n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4302 - acc: 0.8315     \n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4283 - acc: 0.8279     \n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4261 - acc: 0.8306     - ETA: 0s - loss: 0.4262 - acc: 0.829\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4264 - acc: 0.8300     - ETA: 0s - loss: 0.4329 - acc: 0.\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4260 - acc: 0.8276     \n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4259 - acc: 0.8296     \n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4251 - acc: 0.8292     \n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4230 - acc: 0.8319     \n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4299 - acc: 0.8309     \n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4269 - acc: 0.8315     \n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4249 - acc: 0.8309     \n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4299 - acc: 0.8304     \n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4274 - acc: 0.8321     \n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4296 - acc: 0.8312     \n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4309 - acc: 0.8295     \n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4268 - acc: 0.8304     \n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4269 - acc: 0.8300     \n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4209 - acc: 0.8301     \n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4256 - acc: 0.8302     \n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4288 - acc: 0.8307     \n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4283 - acc: 0.8324     \n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4248 - acc: 0.8324     \n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4270 - acc: 0.8324     \n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4274 - acc: 0.8297     - ETA: 0s - loss: 0.4261 - acc\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4254 - acc: 0.8310     \n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4245 - acc: 0.8304     \n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4286 - acc: 0.8316     \n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4289 - acc: 0.8310     \n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4279 - acc: 0.8340     - ETA: 0s - loss: 0.4267 - acc: 0.\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4253 - acc: 0.8320     \n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4287 - acc: 0.8315     \n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4265 - acc: 0.8302     \n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4249 - acc: 0.8305     \n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4253 - acc: 0.8317     - ETA: 0s - loss: 0.4102 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xbc412177f0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the ann to the training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_proba = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2759698 ],\n",
       "       [ 0.3491585 ],\n",
       "       [ 0.24349687],\n",
       "       ..., \n",
       "       [ 0.18496327],\n",
       "       [ 0.13486747],\n",
       "       [ 0.17176798]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       ..., \n",
       "       [False],\n",
       "       [False],\n",
       "       [False]], dtype=bool)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (y_pred_proba > 0.5)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1567,   28],\n",
       "       [ 297,  108]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0xbc416faf60>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAEFCAYAAADqlvKRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbc43376c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cm, square= True, annot = True, cbar = False)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]]\n"
     ]
    }
   ],
   "source": [
    "#Le's try our model for the following customer\n",
    "#Geography: France\n",
    "#Credit score = 600\n",
    "#Gender = Male\n",
    "#Age = 40\n",
    "#Tenure = 3\n",
    "#Balance = 60000\n",
    "#Number of products = 2\n",
    "#Has credit card = Yes\n",
    "#Is active member = Yes\n",
    "#Estimated salary = 50000\n",
    "new_prediction = classifier.predict(sc.transform(np.array([[0.0, 0.0, 600.0, 1.0, 40.0, 3.0, 60000.0, 2.0, 1.0, 1.0, 50000.0]])))\n",
    "new_prediction = (new_prediction > 0.5)\n",
    "print(new_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#K-fold cross validation for a btter evaluation of the model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "    \n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform',\n",
    "                         activation = 'relu', input_dim = 11))\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform',\n",
    "                         activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform',\n",
    "                         activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy',\n",
    "                       metrics = ['accuracy'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = KerasClassifier(build_fn = build_classifier, \n",
    "                            batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.4944 - acc: 0.7967     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4301 - acc: 0.7971     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4250 - acc: 0.7971     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4211 - acc: 0.8011     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4182 - acc: 0.8215     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4161 - acc: 0.8240     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4134 - acc: 0.8294     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4128 - acc: 0.8286     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4116 - acc: 0.8329     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4103 - acc: 0.8318     \n",
      " 10/800 [..............................] - ETA: 12sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.4926 - acc: 0.7964     - ETA: 0s - loss: 0.5102 - acc: 0\n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4310 - acc: 0.7967     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4267 - acc: 0.7967     - ETA: 0s - loss: 0.4312 - acc: \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4220 - acc: 0.7967     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4186 - acc: 0.8214     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4161 - acc: 0.8256     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4144 - acc: 0.8286     - ETA: 0s - loss: 0.4157 - acc: 0.\n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4133 - acc: 0.8308     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4120 - acc: 0.8310     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4111 - acc: 0.8314     \n",
      "610/800 [=====================>........] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.5025 - acc: 0.7950     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4316 - acc: 0.7956     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4280 - acc: 0.7956     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4243 - acc: 0.8149     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4192 - acc: 0.8243     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4148 - acc: 0.8283     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4111 - acc: 0.8299     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4081 - acc: 0.8329     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4058 - acc: 0.8328     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4046 - acc: 0.8339     \n",
      " 10/800 [..............................] - ETA: 12sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.4803 - acc: 0.7975     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4256 - acc: 0.7975     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4198 - acc: 0.8011     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4152 - acc: 0.8279     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4130 - acc: 0.8279     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4116 - acc: 0.8324     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4098 - acc: 0.8336     - ETA: 0s - loss: 0.4097 - acc: \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4088 - acc: 0.8347     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4078 - acc: 0.8346     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4064 - acc: 0.8349     \n",
      " 10/800 [..............................] - ETA: 15sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.4968 - acc: 0.7935     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4174 - acc: 0.8185     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4028 - acc: 0.8260     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3929 - acc: 0.8278     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3863 - acc: 0.8278     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3814 - acc: 0.8325     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3765 - acc: 0.8442     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3735 - acc: 0.8443     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3702 - acc: 0.8464     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3674 - acc: 0.8492     \n",
      " 10/800 [..............................] - ETA: 12sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.4948 - acc: 0.7940     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4297 - acc: 0.7944     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4239 - acc: 0.7944     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4195 - acc: 0.8143     - ETA: 0s - loss: 0.4232 - acc: \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4173 - acc: 0.8250     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4151 - acc: 0.8281     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4139 - acc: 0.8317     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4118 - acc: 0.8331     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4108 - acc: 0.8321     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4097 - acc: 0.8342     \n",
      "670/800 [========================>.....] - ETA: 0s Epoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.4837 - acc: 0.7967     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4282 - acc: 0.7969     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4242 - acc: 0.7969     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4206 - acc: 0.8015     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4181 - acc: 0.8237     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4162 - acc: 0.8258     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4147 - acc: 0.8292     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4133 - acc: 0.8307     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4124 - acc: 0.8317     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4116 - acc: 0.8332     \n",
      "570/800 [====================>.........] - ETA: 0s Epoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.4920 - acc: 0.7983     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4103 - acc: 0.8232     - ETA: 0s - loss: 0.4119 - acc: 0.82\n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3966 - acc: 0.8294     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3869 - acc: 0.8300     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3801 - acc: 0.8318     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3736 - acc: 0.8393     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3704 - acc: 0.8450     - ETA: 0s - loss: 0.3741 - acc: 0\n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3667 - acc: 0.8497     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3642 - acc: 0.8508     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3615 - acc: 0.8507     \n",
      "730/800 [==========================>...] - ETA: 0s Epoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.4871 - acc: 0.7951     - ETA: 0s - loss: 0.5345 - acc:\n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4231 - acc: 0.7957     - ETA: 0s - loss: 0.4239 - acc: 0.796\n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4162 - acc: 0.8239     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4106 - acc: 0.8312     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4065 - acc: 0.8324     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4035 - acc: 0.8356     - ETA: 0s - loss: 0.4065 - acc: 0.\n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4021 - acc: 0.8354     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4002 - acc: 0.8361     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3988 - acc: 0.8361     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3987 - acc: 0.8354     \n",
      "690/800 [========================>.....] - ETA: 0s Epoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.4902 - acc: 0.7961     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4304 - acc: 0.7961     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4263 - acc: 0.7961     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4230 - acc: 0.8056     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4200 - acc: 0.8218     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4187 - acc: 0.8262     - ETA: 0s - loss: 0.4186 - acc\n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4170 - acc: 0.8281     - ETA: 0s - loss: 0.4151 - acc: \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4157 - acc: 0.8307     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4143 - acc: 0.8307     - ETA: 0s - loss: 0.4207 - acc: \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4137 - acc: 0.8315     \n",
      "740/800 [==========================>...] - ETA: 0s "
     ]
    }
   ],
   "source": [
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  0.834999995567\n",
      "variance:  0.0128937964461\n"
     ]
    }
   ],
   "source": [
    "print(\"mean: \", accuracies.mean())\n",
    "print(\"variance: \", accuracies.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Improving the ANN with parameter tuning\n",
    "#Dropout Regularization to reduce overfitting if needed\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    \n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform',\n",
    "                         activation = 'relu', input_dim = 11))\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform',\n",
    "                         activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform',\n",
    "                         activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy',\n",
    "                       metrics = ['accuracy'])\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = KerasClassifier(build_fn = build_classifier)\n",
    "\n",
    "parameters = {'batch_size': [25, 32],\n",
    "             'nb_epoch': [100, 500],\n",
    "             'optimizer': ['adam', 'rmsprop']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator= classifier, \n",
    "                          param_grid = parameters,\n",
    "                          scoring = 'accuracy',\n",
    "                          cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.5418 - acc: 0.7971     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4335 - acc: 0.7971     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4278 - acc: 0.7971     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4242 - acc: 0.7971     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4208 - acc: 0.7971     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4188 - acc: 0.8099     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4171 - acc: 0.8225     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4157 - acc: 0.8250     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4148 - acc: 0.8256     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4141 - acc: 0.8285     \n",
      "7025/7200 [============================>.] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.5415 - acc: 0.7963     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4321 - acc: 0.7967     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4261 - acc: 0.7967     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4219 - acc: 0.7967     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4172 - acc: 0.8172     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4138 - acc: 0.8265     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4107 - acc: 0.8304     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4089 - acc: 0.8311     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4068 - acc: 0.8321     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4055 - acc: 0.8343     \n",
      "7150/7200 [============================>.] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.5525 - acc: 0.7946     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4379 - acc: 0.7956     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4311 - acc: 0.7956     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4280 - acc: 0.7956     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4249 - acc: 0.7956     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4221 - acc: 0.7956     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4203 - acc: 0.8137     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4187 - acc: 0.8200     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4171 - acc: 0.8235     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4162 - acc: 0.8262     \n",
      "7000/7200 [============================>.] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.5496 - acc: 0.7962     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4215 - acc: 0.7975     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4121 - acc: 0.7975     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4060 - acc: 0.7975     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3998 - acc: 0.8008     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3946 - acc: 0.8306     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3896 - acc: 0.8307     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3849 - acc: 0.8322     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3808 - acc: 0.8335     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3769 - acc: 0.8336     \n",
      "6900/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.5933 - acc: 0.7925     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4415 - acc: 0.7938     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4345 - acc: 0.7938     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4316 - acc: 0.7937     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4290 - acc: 0.7937     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4263 - acc: 0.7938     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4240 - acc: 0.7938     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4222 - acc: 0.8131     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4206 - acc: 0.8217     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4195 - acc: 0.8244     \n",
      "6200/7200 [========================>.....] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.5640 - acc: 0.7926     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4363 - acc: 0.7944     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4292 - acc: 0.7944     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4253 - acc: 0.7944     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4225 - acc: 0.7944     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4200 - acc: 0.8072     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4185 - acc: 0.8229     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4170 - acc: 0.8256     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4160 - acc: 0.8276     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4148 - acc: 0.8281     \n",
      "7200/7200 [==============================] - 0s     \n",
      "Epoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.5616 - acc: 0.7961     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4376 - acc: 0.7969     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4309 - acc: 0.7969     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4284 - acc: 0.7969     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4261 - acc: 0.7969     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4234 - acc: 0.8015     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4193 - acc: 0.8215     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4162 - acc: 0.8279     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4125 - acc: 0.8326     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4100 - acc: 0.8335     \n",
      "6375/7200 [=========================>....] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.6550 - acc: 0.7956     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5980 - acc: 0.7963     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5610 - acc: 0.7962     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5374 - acc: 0.7963     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5232 - acc: 0.7962     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5148 - acc: 0.7963     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5101 - acc: 0.7962     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5077 - acc: 0.7962     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - ETA: 0s - loss: 0.5075 - acc: 0.795 - 0s - loss: 0.5066 - acc: 0.7962     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5060 - acc: 0.7962     \n",
      "6950/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.5706 - acc: 0.7954     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4349 - acc: 0.7957     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4278 - acc: 0.7957     \n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 0s - loss: 0.4247 - acc: 0.7957     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4224 - acc: 0.7957     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4195 - acc: 0.8186     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4163 - acc: 0.8250     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4128 - acc: 0.8282     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4099 - acc: 0.8318     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4069 - acc: 0.8343     \n",
      "6875/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5611 - acc: 0.7946     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4398 - acc: 0.7961     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4333 - acc: 0.7961     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4317 - acc: 0.7961     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4295 - acc: 0.7961     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4277 - acc: 0.7961     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4254 - acc: 0.7961     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4230 - acc: 0.8007     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4213 - acc: 0.8168     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4204 - acc: 0.8186     \n",
      "5825/7200 [=======================>......] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.6133 - acc: 0.7950     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4591 - acc: 0.7971     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4374 - acc: 0.7971     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4312 - acc: 0.7971     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4278 - acc: 0.7971     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4251 - acc: 0.7971     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4228 - acc: 0.7971     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4203 - acc: 0.7971     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4186 - acc: 0.8104     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4173 - acc: 0.8228     \n",
      "6750/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.5616 - acc: 0.7968     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4393 - acc: 0.7967     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4300 - acc: 0.7967     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4265 - acc: 0.7967     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4233 - acc: 0.7967     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4207 - acc: 0.8015     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4185 - acc: 0.8211     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4169 - acc: 0.8247     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4155 - acc: 0.8275     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4146 - acc: 0.8269     \n",
      "6425/7200 [=========================>....] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.5854 - acc: 0.7950     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4448 - acc: 0.7956     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4320 - acc: 0.7956     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4267 - acc: 0.7956     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4230 - acc: 0.7956     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4194 - acc: 0.8214     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4164 - acc: 0.8297     - ETA: 0s - loss: 0.4142 - ac\n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4137 - acc: 0.8300     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4110 - acc: 0.8340     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4092 - acc: 0.8347     \n",
      "6775/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.5494 - acc: 0.7975     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4363 - acc: 0.7975     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4290 - acc: 0.7975     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4248 - acc: 0.7975     - ETA: 0s - loss: 0.4185 - \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4215 - acc: 0.7975     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4185 - acc: 0.8010     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4165 - acc: 0.8212     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4149 - acc: 0.8231     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4135 - acc: 0.8274     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4122 - acc: 0.8310     \n",
      "6725/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.5927 - acc: 0.7936     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4480 - acc: 0.7938     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4312 - acc: 0.7937     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4218 - acc: 0.7942     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4121 - acc: 0.8043     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4014 - acc: 0.8171     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3899 - acc: 0.8362     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3782 - acc: 0.8451     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3692 - acc: 0.8519     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3628 - acc: 0.8533     \n",
      "6575/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.5740 - acc: 0.7936     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4424 - acc: 0.7944     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4331 - acc: 0.7944     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4297 - acc: 0.7944     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4278 - acc: 0.7944     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4256 - acc: 0.7944     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4231 - acc: 0.8033     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4212 - acc: 0.8178     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4194 - acc: 0.8219     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4180 - acc: 0.8250     \n",
      "6750/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.5706 - acc: 0.7956     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4427 - acc: 0.7969     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4329 - acc: 0.7969     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4298 - acc: 0.7969     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4271 - acc: 0.7969     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4242 - acc: 0.7969     \n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 0s - loss: 0.4218 - acc: 0.7969     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4193 - acc: 0.8096     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4178 - acc: 0.8206     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4165 - acc: 0.8250     \n",
      "7100/7200 [============================>.] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5931 - acc: 0.7951     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4457 - acc: 0.7962     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4258 - acc: 0.7964     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4189 - acc: 0.8154     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4129 - acc: 0.8219     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4073 - acc: 0.8228     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4015 - acc: 0.8276     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3957 - acc: 0.8290     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3908 - acc: 0.8279     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3875 - acc: 0.8312     \n",
      "6325/7200 [=========================>....] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 1s - loss: 0.5694 - acc: 0.7950     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4399 - acc: 0.7957     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4297 - acc: 0.7957     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4257 - acc: 0.7957     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4224 - acc: 0.7957     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4199 - acc: 0.7965     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4173 - acc: 0.8192     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4155 - acc: 0.8239     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4141 - acc: 0.8250     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4126 - acc: 0.8292     \n",
      "6950/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5999 - acc: 0.7956     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4536 - acc: 0.7961     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4365 - acc: 0.7961     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4316 - acc: 0.7961     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4284 - acc: 0.7961     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4256 - acc: 0.7961     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4234 - acc: 0.7961     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4214 - acc: 0.8035     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4197 - acc: 0.8192     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4183 - acc: 0.8244     \n",
      "6400/7200 [=========================>....] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5821 - acc: 0.7953     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4306 - acc: 0.7971     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4147 - acc: 0.8162     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4043 - acc: 0.8276     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3967 - acc: 0.8289     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3902 - acc: 0.8300     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3855 - acc: 0.8314     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3814 - acc: 0.8311     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3781 - acc: 0.8347     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3753 - acc: 0.8329     \n",
      "6500/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5539 - acc: 0.7963     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4339 - acc: 0.7967     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4284 - acc: 0.7967     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4254 - acc: 0.7967     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4224 - acc: 0.7967     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4200 - acc: 0.7997     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4183 - acc: 0.8206     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4171 - acc: 0.8221     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4163 - acc: 0.8267     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4153 - acc: 0.8278     \n",
      "6375/7200 [=========================>....] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5658 - acc: 0.7942     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4352 - acc: 0.7956     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4318 - acc: 0.7956     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4296 - acc: 0.7956     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4279 - acc: 0.7956     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4258 - acc: 0.7956     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4234 - acc: 0.7956     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4213 - acc: 0.8090     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4194 - acc: 0.8193     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4184 - acc: 0.8224     \n",
      "6250/7200 [=========================>....] - ETA:  - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5454 - acc: 0.7975     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4348 - acc: 0.7975     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4295 - acc: 0.7975     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4265 - acc: 0.7975     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4232 - acc: 0.7975     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4200 - acc: 0.7975     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4176 - acc: 0.8115     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4160 - acc: 0.8233     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4143 - acc: 0.8260     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4133 - acc: 0.8282     \n",
      "6525/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5455 - acc: 0.7938     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4411 - acc: 0.7938     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4338 - acc: 0.7938     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4297 - acc: 0.7938     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4261 - acc: 0.7938     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4236 - acc: 0.8040     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4214 - acc: 0.8196     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4200 - acc: 0.8208     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4192 - acc: 0.8256     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4182 - acc: 0.8253     \n",
      "6425/7200 [=========================>....] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5656 - acc: 0.7924     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4369 - acc: 0.7944     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4315 - acc: 0.7944     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4275 - acc: 0.7944     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4238 - acc: 0.7979     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4196 - acc: 0.8204     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4156 - acc: 0.8264     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4125 - acc: 0.8294     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4096 - acc: 0.8312     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4072 - acc: 0.8335     \n",
      "7175/7200 [============================>.] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5414 - acc: 0.7969     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4366 - acc: 0.7969     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4323 - acc: 0.7969     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4305 - acc: 0.7969     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4289 - acc: 0.7969     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4277 - acc: 0.7969     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4261 - acc: 0.7969     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4234 - acc: 0.7969     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4210 - acc: 0.8157     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4193 - acc: 0.8204     \n",
      "6450/7200 [=========================>....] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5545 - acc: 0.7964     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4355 - acc: 0.7962     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4306 - acc: 0.7963     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4277 - acc: 0.7962     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4249 - acc: 0.7962     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4215 - acc: 0.7963     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4188 - acc: 0.8101     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4167 - acc: 0.8203     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4151 - acc: 0.8251     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4139 - acc: 0.8265     \n",
      "6700/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5823 - acc: 0.7937     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4345 - acc: 0.7957     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4269 - acc: 0.7957     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4224 - acc: 0.7957     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4185 - acc: 0.8068     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4143 - acc: 0.8276     - ETA: 0s - loss: 0.4252 - a\n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4108 - acc: 0.8318     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4078 - acc: 0.8349     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4052 - acc: 0.8369     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4031 - acc: 0.8361     \n",
      "6400/7200 [=========================>....] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5487 - acc: 0.7962     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4359 - acc: 0.7961     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4305 - acc: 0.7961     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4274 - acc: 0.7961     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4245 - acc: 0.7961     - ETA: 0s - loss: 0.4156 - acc:\n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4224 - acc: 0.8043     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4209 - acc: 0.8171     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4194 - acc: 0.8214     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4184 - acc: 0.8228     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4174 - acc: 0.8256     \n",
      "7000/7200 [============================>.] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6109 - acc: 0.7958     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4563 - acc: 0.7971     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4262 - acc: 0.7972     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4160 - acc: 0.8174     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4075 - acc: 0.8278     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4009 - acc: 0.8296     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3952 - acc: 0.8297     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3906 - acc: 0.8325     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3862 - acc: 0.8304     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3830 - acc: 0.8357     \n",
      "6450/7200 [=========================>....] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5883 - acc: 0.7950     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4451 - acc: 0.7967     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4333 - acc: 0.7967     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4293 - acc: 0.7967     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4268 - acc: 0.7967     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4245 - acc: 0.7967     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4219 - acc: 0.7967     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4197 - acc: 0.8121     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4180 - acc: 0.8196     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4167 - acc: 0.8257     \n",
      "6950/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5661 - acc: 0.7954     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4411 - acc: 0.7956     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4327 - acc: 0.7956     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4294 - acc: 0.7956     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4273 - acc: 0.7956     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4245 - acc: 0.8003     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4212 - acc: 0.8215     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4180 - acc: 0.8258     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4148 - acc: 0.8314     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4121 - acc: 0.8321     \n",
      "6550/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5873 - acc: 0.7940     \n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 0s - loss: 0.4382 - acc: 0.7975     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4242 - acc: 0.7978     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4192 - acc: 0.8193     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4143 - acc: 0.8265     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4093 - acc: 0.8293     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4044 - acc: 0.8312     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3980 - acc: 0.8322     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3919 - acc: 0.8332     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3867 - acc: 0.8331     \n",
      "6250/7200 [=========================>....] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5879 - acc: 0.7937     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4501 - acc: 0.7938     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4365 - acc: 0.7937     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4314 - acc: 0.7937     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4279 - acc: 0.7937     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4249 - acc: 0.7937     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4225 - acc: 0.8139     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4207 - acc: 0.8224     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4192 - acc: 0.8243     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4176 - acc: 0.8262     \n",
      "800/800 [==============================] - 1s      \n",
      "7075/7200 [============================>.] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6002 - acc: 0.7933     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4523 - acc: 0.7944     - ETA: 0s - loss: 0.4502 - acc: 0\n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4357 - acc: 0.7944     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4312 - acc: 0.7944     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4287 - acc: 0.7944     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4274 - acc: 0.7944     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4253 - acc: 0.7944     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4232 - acc: 0.7944     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4212 - acc: 0.8142     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4198 - acc: 0.8193     \n",
      "6900/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6257 - acc: 0.7953     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4794 - acc: 0.7969     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4376 - acc: 0.7969     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4245 - acc: 0.8018     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4131 - acc: 0.8111     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4019 - acc: 0.8153     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3913 - acc: 0.8172     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3819 - acc: 0.8250     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3728 - acc: 0.8456     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3652 - acc: 0.8507     \n",
      "6725/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5827 - acc: 0.7944     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4437 - acc: 0.7962     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4294 - acc: 0.7963     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4238 - acc: 0.7963     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4193 - acc: 0.8081     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4150 - acc: 0.8251     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4113 - acc: 0.8325     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4085 - acc: 0.8321     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4062 - acc: 0.8331     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4042 - acc: 0.8353     \n",
      "800/800 [==============================] - 1s      \n",
      "6925/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6108 - acc: 0.7937     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4525 - acc: 0.7957     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4336 - acc: 0.7957     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4286 - acc: 0.7957     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4261 - acc: 0.7957     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4242 - acc: 0.7957     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4216 - acc: 0.7962     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4184 - acc: 0.8236     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4151 - acc: 0.8264     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4120 - acc: 0.8300     \n",
      "6900/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6085 - acc: 0.7946     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4587 - acc: 0.7961     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4287 - acc: 0.7969     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4147 - acc: 0.8089     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4023 - acc: 0.8161     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3905 - acc: 0.8186     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3798 - acc: 0.8206     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3714 - acc: 0.8428     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3648 - acc: 0.8503     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3596 - acc: 0.8532     \n",
      "6525/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5908 - acc: 0.7964     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4394 - acc: 0.7971     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4313 - acc: 0.7971     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4288 - acc: 0.7971     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4269 - acc: 0.7971     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4247 - acc: 0.7971     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4223 - acc: 0.7971     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4200 - acc: 0.7971     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4184 - acc: 0.8165     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4171 - acc: 0.8215     \n",
      "6624/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5894 - acc: 0.7947     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4423 - acc: 0.7967     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4321 - acc: 0.7967     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4284 - acc: 0.7967     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4263 - acc: 0.7967     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4241 - acc: 0.7971     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4219 - acc: 0.8083     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4186 - acc: 0.8233     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4159 - acc: 0.8281     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4126 - acc: 0.8311     \n",
      "7008/7200 [============================>.] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6036 - acc: 0.7942     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4423 - acc: 0.7956     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4308 - acc: 0.7956     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4256 - acc: 0.7956     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4214 - acc: 0.8043     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4179 - acc: 0.8215     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4147 - acc: 0.8282     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4121 - acc: 0.8308     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4097 - acc: 0.8318     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4078 - acc: 0.8339     \n",
      "6560/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6065 - acc: 0.7964     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4337 - acc: 0.8004     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4145 - acc: 0.8206     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4052 - acc: 0.8239     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3978 - acc: 0.8265     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3922 - acc: 0.8286     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3878 - acc: 0.8294     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3839 - acc: 0.8314     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3801 - acc: 0.8322     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3768 - acc: 0.8340     \n",
      "6368/7200 [=========================>....] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6644 - acc: 0.7908     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.6164 - acc: 0.7937     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5813 - acc: 0.7937     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5565 - acc: 0.7937     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5393 - acc: 0.7937     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5278 - acc: 0.7937     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5203 - acc: 0.7937     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5156 - acc: 0.7937     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5127 - acc: 0.7937     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5110 - acc: 0.7937     \n",
      "6848/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5733 - acc: 0.7935     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4391 - acc: 0.7944     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4311 - acc: 0.7944     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4269 - acc: 0.7944     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4226 - acc: 0.7968     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4189 - acc: 0.8211     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4152 - acc: 0.8281     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4127 - acc: 0.8304     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4100 - acc: 0.8335     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4081 - acc: 0.8332     \n",
      "6432/7200 [=========================>....] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5770 - acc: 0.7968     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4375 - acc: 0.7969     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4310 - acc: 0.7969     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4278 - acc: 0.7969     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4250 - acc: 0.7969     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4227 - acc: 0.7969     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4204 - acc: 0.7969     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4185 - acc: 0.8147     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4173 - acc: 0.8215     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4161 - acc: 0.8247     \n",
      "6848/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5750 - acc: 0.7962     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4379 - acc: 0.7962     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4302 - acc: 0.7962     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4263 - acc: 0.7962     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4229 - acc: 0.7962     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4200 - acc: 0.7962     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4173 - acc: 0.8060     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4161 - acc: 0.8207     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4151 - acc: 0.8249     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4141 - acc: 0.8267     \n",
      "6816/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6094 - acc: 0.7931     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4400 - acc: 0.7965     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4138 - acc: 0.8074     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3976 - acc: 0.8156     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3851 - acc: 0.8186     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3739 - acc: 0.8414     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3645 - acc: 0.8499     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3572 - acc: 0.8533     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3517 - acc: 0.8565     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3484 - acc: 0.8585     \n",
      "800/800 [==============================] - 1s      \n",
      "6944/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5937 - acc: 0.7961     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4389 - acc: 0.7961     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4246 - acc: 0.8110     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4167 - acc: 0.8207     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4093 - acc: 0.8237     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4035 - acc: 0.8244     \n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 0s - loss: 0.3982 - acc: 0.8254     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3932 - acc: 0.8271     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3891 - acc: 0.8289     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3849 - acc: 0.8299     \n",
      "6272/7200 [=========================>....] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6144 - acc: 0.7951     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4616 - acc: 0.7971     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4369 - acc: 0.7971     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4314 - acc: 0.7971     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4279 - acc: 0.7971     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4249 - acc: 0.7971     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4218 - acc: 0.7971     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4192 - acc: 0.8164     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4164 - acc: 0.8224     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4141 - acc: 0.8271     \n",
      "6272/7200 [=========================>....] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6128 - acc: 0.7964     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4597 - acc: 0.7967     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4329 - acc: 0.7967     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4268 - acc: 0.7967     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4242 - acc: 0.7967     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4221 - acc: 0.7967     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4200 - acc: 0.7967     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4179 - acc: 0.7967     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4155 - acc: 0.8126     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4129 - acc: 0.8246     \n",
      "7104/7200 [============================>.] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5950 - acc: 0.7942     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4522 - acc: 0.7956     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4361 - acc: 0.7956     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4305 - acc: 0.7956     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4278 - acc: 0.7956     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4250 - acc: 0.7956     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4228 - acc: 0.7956     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4206 - acc: 0.8014     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4190 - acc: 0.8210     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4179 - acc: 0.8242     \n",
      "6816/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6235 - acc: 0.7950     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4652 - acc: 0.7975     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4313 - acc: 0.7975     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4229 - acc: 0.7975     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4174 - acc: 0.7975     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4120 - acc: 0.8169     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4060 - acc: 0.8268     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3999 - acc: 0.8299     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3942 - acc: 0.8314     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3890 - acc: 0.8325     \n",
      "6464/7200 [=========================>....] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6169 - acc: 0.7937     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4643 - acc: 0.7937     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4399 - acc: 0.7937     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4352 - acc: 0.7937     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4328 - acc: 0.7937     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4304 - acc: 0.7937     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4281 - acc: 0.7937     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4260 - acc: 0.7937     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4239 - acc: 0.7997     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4223 - acc: 0.8189     \n",
      "800/800 [==============================] - 1s      \n",
      "6688/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5953 - acc: 0.7943     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4504 - acc: 0.7944     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4351 - acc: 0.7944     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4294 - acc: 0.7944     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4260 - acc: 0.7944     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4234 - acc: 0.7944     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4209 - acc: 0.7968     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4190 - acc: 0.8206     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4173 - acc: 0.8233     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4164 - acc: 0.8271     \n",
      "800/800 [==============================] - 1s      \n",
      "6720/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6130 - acc: 0.7956     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4578 - acc: 0.7969     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4357 - acc: 0.7969     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4301 - acc: 0.7969     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4266 - acc: 0.7969     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4234 - acc: 0.7969     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4208 - acc: 0.8025     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4180 - acc: 0.8261     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4156 - acc: 0.8279     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4134 - acc: 0.8307     \n",
      "6848/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6235 - acc: 0.7932     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4703 - acc: 0.7962     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4368 - acc: 0.7962     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4312 - acc: 0.7962     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4276 - acc: 0.7962     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4250 - acc: 0.7962     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4228 - acc: 0.7962     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4209 - acc: 0.7962     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4190 - acc: 0.7962     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4173 - acc: 0.8210     \n",
      "800/800 [==============================] - 1s      \n",
      "6752/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6013 - acc: 0.7942     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4489 - acc: 0.7957     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4322 - acc: 0.7957     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4273 - acc: 0.7957     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4247 - acc: 0.7957     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4222 - acc: 0.7957     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4196 - acc: 0.8025     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4168 - acc: 0.8244     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4142 - acc: 0.8294     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4118 - acc: 0.8292     \n",
      "6816/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6087 - acc: 0.7961     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4559 - acc: 0.7961     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4366 - acc: 0.7961     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4314 - acc: 0.7961     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4288 - acc: 0.7961     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4263 - acc: 0.7961     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4239 - acc: 0.7961     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4211 - acc: 0.8143     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4190 - acc: 0.8251     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4170 - acc: 0.8285     \n",
      "800/800 [==============================] - 1s      \n",
      "6656/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5886 - acc: 0.7965     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4390 - acc: 0.7971     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4321 - acc: 0.7971     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4297 - acc: 0.7971     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4281 - acc: 0.7971     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4270 - acc: 0.7971     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4256 - acc: 0.7971     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4241 - acc: 0.7971     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4226 - acc: 0.7971     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4210 - acc: 0.8151     \n",
      "800/800 [==============================] - 1s      \n",
      "7200/7200 [==============================] - 0s     \n",
      "Epoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6643 - acc: 0.7940     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.6157 - acc: 0.7967     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5798 - acc: 0.7967     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5543 - acc: 0.7967     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5368 - acc: 0.7967     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5249 - acc: 0.7967     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5171 - acc: 0.7967     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5122 - acc: 0.7967     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5091 - acc: 0.7967     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5072 - acc: 0.7967     \n",
      "6304/7200 [=========================>....] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5858 - acc: 0.7944     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4416 - acc: 0.7956     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4322 - acc: 0.7956     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4290 - acc: 0.7956     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4265 - acc: 0.7956     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4241 - acc: 0.7956     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4219 - acc: 0.7956     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4196 - acc: 0.8093     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4188 - acc: 0.8231     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4174 - acc: 0.8269     \n",
      "7200/7200 [==============================] - 0s     \n",
      "Epoch 1/10\n",
      "7200/7200 [==============================] - 4s - loss: 0.6124 - acc: 0.7946     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4416 - acc: 0.7975     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4244 - acc: 0.7975     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4177 - acc: 0.7975     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4115 - acc: 0.7975     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4031 - acc: 0.8018     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3879 - acc: 0.8175     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3737 - acc: 0.8304     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3633 - acc: 0.8475     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3571 - acc: 0.8524     \n",
      "6592/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5774 - acc: 0.7937     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4421 - acc: 0.7937     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4347 - acc: 0.7937     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4312 - acc: 0.7937     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4285 - acc: 0.7937     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4255 - acc: 0.7937     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4230 - acc: 0.7989     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4217 - acc: 0.8186     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4202 - acc: 0.8214     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4190 - acc: 0.8256     \n",
      "7104/7200 [============================>.] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5678 - acc: 0.7944     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4376 - acc: 0.7944     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4319 - acc: 0.7944     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4294 - acc: 0.7944     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4276 - acc: 0.7944     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4256 - acc: 0.7944     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4233 - acc: 0.7944     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4209 - acc: 0.8076     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4197 - acc: 0.8201     \n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 0s - loss: 0.4186 - acc: 0.8210     \n",
      "800/800 [==============================] - 1s      \n",
      "7136/7200 [============================>.] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5786 - acc: 0.7956     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4376 - acc: 0.7969     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4308 - acc: 0.7969     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4287 - acc: 0.7969     - ETA: 0s - loss: 0.4312 - a\n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4267 - acc: 0.7969     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4246 - acc: 0.7969     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4228 - acc: 0.7969     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4205 - acc: 0.8043     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4190 - acc: 0.8203     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4175 - acc: 0.8210     \n",
      "6496/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5894 - acc: 0.7950     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4374 - acc: 0.7962     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4284 - acc: 0.7962     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4234 - acc: 0.7962     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4194 - acc: 0.8115     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4155 - acc: 0.8258     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4124 - acc: 0.8296     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4092 - acc: 0.8313     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4069 - acc: 0.8317     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4054 - acc: 0.8333     \n",
      "6912/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5824 - acc: 0.7942     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4383 - acc: 0.7957     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4289 - acc: 0.7957     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4241 - acc: 0.7957     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4197 - acc: 0.7976     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4156 - acc: 0.8260     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4121 - acc: 0.8310     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4086 - acc: 0.8344     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4059 - acc: 0.8368     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4037 - acc: 0.8367     \n",
      "6560/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6349 - acc: 0.7949     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4571 - acc: 0.7974     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4150 - acc: 0.8118     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3968 - acc: 0.8183     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3840 - acc: 0.8211     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3737 - acc: 0.8436     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3655 - acc: 0.8503     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3596 - acc: 0.8571     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3549 - acc: 0.8579     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3521 - acc: 0.8586     \n",
      "6528/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6501 - acc: 0.7949     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.5244 - acc: 0.7971     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4478 - acc: 0.7971     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4369 - acc: 0.7971     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4331 - acc: 0.7971     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4307 - acc: 0.7971     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4291 - acc: 0.7971     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4277 - acc: 0.7971     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4262 - acc: 0.7971     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4248 - acc: 0.7971     \n",
      "6656/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5958 - acc: 0.7954     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4482 - acc: 0.7967     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4325 - acc: 0.7967     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4274 - acc: 0.7967     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4237 - acc: 0.7967     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4201 - acc: 0.8022     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4171 - acc: 0.8253     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4141 - acc: 0.8275     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4119 - acc: 0.8306     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4098 - acc: 0.8318     \n",
      "6560/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6259 - acc: 0.7942     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4735 - acc: 0.7956     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4404 - acc: 0.7956     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4340 - acc: 0.7956     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4311 - acc: 0.7956     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4291 - acc: 0.7956     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4273 - acc: 0.7956     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4255 - acc: 0.7956     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4231 - acc: 0.7956     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4212 - acc: 0.7962     \n",
      "800/800 [==============================] - 1s      \n",
      "6336/7200 [=========================>....] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5956 - acc: 0.7961     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4475 - acc: 0.7975     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4317 - acc: 0.7975     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4258 - acc: 0.7975     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4220 - acc: 0.7975     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4189 - acc: 0.7975     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4163 - acc: 0.8104     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4144 - acc: 0.8222     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4134 - acc: 0.8278     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4122 - acc: 0.8290     \n",
      "6880/7200 [===========================>..] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6208 - acc: 0.7925     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4701 - acc: 0.7937     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4436 - acc: 0.7937     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4368 - acc: 0.7937     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4329 - acc: 0.7937     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4306 - acc: 0.7937     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4271 - acc: 0.7937     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4241 - acc: 0.8040     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4210 - acc: 0.8229     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4185 - acc: 0.8268     \n",
      "800/800 [==============================] - 1s      \n",
      "6560/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5869 - acc: 0.7940     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4457 - acc: 0.7944     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4271 - acc: 0.7944     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4206 - acc: 0.7944     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4155 - acc: 0.7944     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4106 - acc: 0.8003     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4055 - acc: 0.8261     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4010 - acc: 0.8283     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3967 - acc: 0.8282     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3924 - acc: 0.8293     \n",
      "6688/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 3s - loss: 0.6351 - acc: 0.7946     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4843 - acc: 0.7969     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4361 - acc: 0.7969     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4244 - acc: 0.8024     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4175 - acc: 0.8211     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4116 - acc: 0.8226     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4055 - acc: 0.8242     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3998 - acc: 0.8268     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3950 - acc: 0.8267     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.3908 - acc: 0.8296     \n",
      "6656/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6009 - acc: 0.7951     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4524 - acc: 0.7962     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4358 - acc: 0.7962     - ETA: 0s - loss: 0.4372 - acc: 0.795\n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4309 - acc: 0.7962     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4284 - acc: 0.7962     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4268 - acc: 0.7962     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4248 - acc: 0.7962     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4227 - acc: 0.7962     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4204 - acc: 0.7992     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4181 - acc: 0.8186     \n",
      "800/800 [==============================] - 1s      \n",
      "7168/7200 [============================>.] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.6005 - acc: 0.7937     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4503 - acc: 0.7957     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4333 - acc: 0.7957     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4289 - acc: 0.7957     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4264 - acc: 0.7957     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4249 - acc: 0.7957     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4233 - acc: 0.7957     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4217 - acc: 0.7957     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4194 - acc: 0.8101     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4177 - acc: 0.8204     \n",
      "800/800 [==============================] - 1s      \n",
      "6400/7200 [=========================>....] - ETA: 0sEpoch 1/10\n",
      "7200/7200 [==============================] - 2s - loss: 0.5985 - acc: 0.7961     \n",
      "Epoch 2/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4506 - acc: 0.7961     \n",
      "Epoch 3/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4358 - acc: 0.7961     \n",
      "Epoch 4/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4315 - acc: 0.7961     \n",
      "Epoch 5/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4286 - acc: 0.7961     \n",
      "Epoch 6/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4264 - acc: 0.7961     \n",
      "Epoch 7/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4241 - acc: 0.7961     \n",
      "Epoch 8/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4221 - acc: 0.8044     \n",
      "Epoch 9/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4206 - acc: 0.8183     \n",
      "Epoch 10/10\n",
      "7200/7200 [==============================] - 0s - loss: 0.4195 - acc: 0.8225     \n",
      "6528/7200 [==========================>...] - ETA: 0sEpoch 1/10\n",
      "8000/8000 [==============================] - 3s - loss: 0.5753 - acc: 0.7962     \n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s - loss: 0.4425 - acc: 0.7960     - ETA: 0s - loss: 0.4436 - acc: 0.79\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s - loss: 0.4322 - acc: 0.7960     \n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s - loss: 0.4279 - acc: 0.7960     \n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s - loss: 0.4241 - acc: 0.7960     \n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s - loss: 0.4204 - acc: 0.8154     \n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s - loss: 0.4167 - acc: 0.8274     \n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s - loss: 0.4136 - acc: 0.8301     \n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s - loss: 0.4109 - acc: 0.8317     \n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s - loss: 0.4089 - acc: 0.8311     \n"
     ]
    }
   ],
   "source": [
    "grid_search = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 25, 'nb_epoch': 500, 'optimizer': 'rmsprop'}\n",
      "0.832\n"
     ]
    }
   ],
   "source": [
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "print(best_parameters)\n",
    "print(best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "\n",
    "----------------------\n",
    "\n",
    "----------------------\n",
    "\n",
    "----------------------\n",
    "\n",
    "# Convolutional Neural Network\n",
    "##### What are convoluional Neural Networks?\n",
    "The way we look at things is a bit similar to the one computer looks at things, or every thing there are different features that differentiates it from other things.CNN is probably one of the best algorithms for image classification\n",
    "\n",
    "##### Convolution\n",
    "This is the convolution function\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/convolution.JPG?raw=true)\n",
    "\n",
    "the convolution function multiplies the input image and the feature detector ro form a feature map\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/fd.JPG?raw=true)\n",
    "\n",
    "One better way to preserve different information is we create different filters to create multiple feature maps. In the end the goal of the convolution is to find features in an image while preserving the pixel relationships in this image\n",
    "\n",
    "##### Relu Layer: Rectified Linear Unit\n",
    "In the ANN we applied an activation function to determine the weights. in CNN once we have our convolution layers in almost the same way we apply to them one of the activation functions discussed before that is the Rectifier function\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/relu.JPG?raw=true)\n",
    "\n",
    "The reason we apply the resctifier is to break the linearity and increase the non linearity specially because images are non linear because of the different elements in an image\n",
    "\n",
    "##### Max Pooling\n",
    "What is pooling and why we need it? in order for the CNN to better find features in images, the CNN looks at the imae in different rotations and scaling and distortions to be able to find features no matter how bad or good the image is.... This process is called pooling, there is different kind of pooling one of the best is max pooling. this algo goes through the feature map in a 2x2 grid and record the max value found in that grid. Another benefit from Pooling is that we are not falling into overfitting since we reduce the amount of information. But why max pooling and not another one?\n",
    "\n",
    "##### Flattening\n",
    "Once we have our Pooled feature map, flattening is the processs of flattening it into a column so we can later input it into our NN\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/flattening.JPG?raw=true)\n",
    "\n",
    "So up till now our process is as follows:\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/process.JPG?raw=true)\n",
    "\n",
    "##### Full connection\n",
    "now we're adding a neural network, a fully connected network is called so because hidden layers are all fully connected. in our input layer we have our flattened images which in itself is pretty powerful prediction tool, but we're adding the NN to combine and find new attributes to optimize the results\n",
    "\n",
    "# Implementing a CNN for image classification \"Cats /Dogs\"\n",
    "Dataset not included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initializing the CNN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Step 1: Convolution\n",
    "#rows and columns here are for the feature detector\n",
    "classifier.add(Convolution2D(32, (3, 3), \n",
    "                            input_shape = (64, 64, 3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "#Adding a second convolution layer\n",
    "classifier.add(Convolution2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Flattening\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy',\n",
    "                       metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fitting the CNN to the images\n",
    "#images are in structure datasset/training_set/dogs/dog1.jpg ......\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train set transformation\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.2,\n",
    "                                  zoom_range = 0.2, horizontal_flip = True)\n",
    "#Test set transformation\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(\n",
    "    'data/dataset/training_set', target_size = (64, 64), batch_size = 32,\n",
    "    class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    'data/dataset/test_set', target_size = (64, 64), batch_size = 32, \n",
    "    class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khalil\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \"\"\"\n",
      "C:\\Users\\khalil\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., steps_per_epoch=250, validation_data=<keras.pre..., validation_steps=62, epochs=25)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 63s - loss: 0.6493 - acc: 0.6192 - val_loss: 0.5875 - val_acc: 0.6951\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 60s - loss: 0.5699 - acc: 0.7041 - val_loss: 0.5553 - val_acc: 0.7188\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 60s - loss: 0.5239 - acc: 0.7402 - val_loss: 0.5749 - val_acc: 0.7006\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 59s - loss: 0.5027 - acc: 0.7555 - val_loss: 0.4903 - val_acc: 0.7762\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 59s - loss: 0.4774 - acc: 0.7672 - val_loss: 0.4719 - val_acc: 0.7782\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 57s - loss: 0.4642 - acc: 0.7802 - val_loss: 0.4648 - val_acc: 0.7883\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 56s - loss: 0.4555 - acc: 0.7826 - val_loss: 0.4624 - val_acc: 0.7969\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 56s - loss: 0.4459 - acc: 0.7871 - val_loss: 0.4554 - val_acc: 0.7959\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 56s - loss: 0.4282 - acc: 0.7976 - val_loss: 0.4467 - val_acc: 0.7969\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 56s - loss: 0.4192 - acc: 0.8036 - val_loss: 0.4520 - val_acc: 0.7913\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 59s - loss: 0.4081 - acc: 0.8119 - val_loss: 0.4531 - val_acc: 0.7989\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 64s - loss: 0.3942 - acc: 0.8201 - val_loss: 0.4539 - val_acc: 0.7939\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 58s - loss: 0.3933 - acc: 0.8223 - val_loss: 0.4228 - val_acc: 0.8216\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 56s - loss: 0.3785 - acc: 0.8311 - val_loss: 0.4226 - val_acc: 0.8185\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 56s - loss: 0.3655 - acc: 0.8340 - val_loss: 0.4110 - val_acc: 0.8226\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 59s - loss: 0.3533 - acc: 0.8382 - val_loss: 0.4157 - val_acc: 0.8165\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 62s - loss: 0.3539 - acc: 0.8387 - val_loss: 0.4343 - val_acc: 0.8065\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 60s - loss: 0.3321 - acc: 0.8511 - val_loss: 0.4452 - val_acc: 0.8175\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 60s - loss: 0.3220 - acc: 0.8599 - val_loss: 0.4148 - val_acc: 0.8367\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 62s - loss: 0.3236 - acc: 0.8566 - val_loss: 0.4505 - val_acc: 0.8216\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 60s - loss: 0.3080 - acc: 0.8658 - val_loss: 0.4221 - val_acc: 0.8301\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 60s - loss: 0.3029 - acc: 0.8682 - val_loss: 0.4284 - val_acc: 0.8201\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 61s - loss: 0.2960 - acc: 0.8685 - val_loss: 0.4287 - val_acc: 0.8281\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 62s - loss: 0.2972 - acc: 0.8770 - val_loss: 0.4342 - val_acc: 0.8155\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 61s - loss: 0.2735 - acc: 0.8846 - val_loss: 0.4939 - val_acc: 0.8070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xbca9493b70>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 250, #8000 / 32\n",
    "                         nb_epoch = 25,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The results are'nt bad but there is quite a difference between the acc in\n",
    "#The training set and th test set, what way to make it better is deeper model\n",
    "#We can add another convolutional layer or another NN layer, but here it is better\n",
    "#To add a convolutional layer\n",
    "#After adding a second convolutional layer we can see the much smaller diff\n",
    "#Between test, and training set, we can further optimize by adding another\n",
    "#Convolutional layer with double amout of feature set....\n",
    "#Another way to improve our model is to increase size of images, here \n",
    "#we're using 64x94 images which is pretty low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A new single prediction\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('data/dataset/single_prediction/cat_or_dog_1.jpg',\n",
    "                           target_size = (64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Formatting the image into an array\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 64, 3)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = classifier.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#So cats correspond to 0, and dogs to 1, our model predicted 1 so we got a correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if result[0][0] == 1:\n",
    "    result = 'dog'\n",
    "else:\n",
    "    result = 'cat'\n",
    "    \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An improved model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Images dimensions\n",
    "img_width, img_height = 150, 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create the CNN model\n",
    "#p: Dropour rate\n",
    "#input_shape\n",
    "def create_model(p, input_shape = (32, 32, 3)):\n",
    "    #Initialising the CNN\n",
    "    model = Sequential()\n",
    "    \n",
    "    #Convolution + Pooling\n",
    "    model.add(Conv2D(32, (3, 3), padding = 'same', input_shape = input_shape,\n",
    "                    activator = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    #2nd Convolution + Pooling\n",
    "    model.add(Conv2D(32, (3, 3), padding = 'same', activator = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    #3rd Convolution + Pooling\n",
    "    model.add(Conv2D(64, (3, 3), padding = 'same', activator = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    #4th Convolution + Pooling\n",
    "    model.add(Conv2D(64, (3, 3), padding = 'same', activator = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    \n",
    "    #Flattening\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    #Fully Connection\n",
    "    model.add(Dense(64, activation = 'relu'))\n",
    "    model.add(Dropout(p))\n",
    "    model.add(Dense(64, activation = 'relu'))\n",
    "    model.add(Dense(64, activation = 'relu'))\n",
    "    model.add(Dropout(p/2))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    #Compiling the CNN\n",
    "    optimizer = Adam(lr=1e-3)\n",
    "    metrics = ['accuracy']\n",
    "    model.compile(optimizer = optimizer, loss = 'binary_crossentropy', \n",
    "                 metrics = metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_training(bs = 32, epochs = 10):\n",
    "    train_dt = ImageDataGenerator(rescale = 1./255,\n",
    "                                  shear_range = 0.2,\n",
    "                                  zoom_range = 0.2,\n",
    "                                  horizontal_flip = True)\n",
    "    test_dt = ImageDataGenerator(rescale = 1./255)\n",
    "    \n",
    "    train_set = train_dt.flow_from_directory('data/dataset/training_set',\n",
    "                                            target_size = (img_width, img_height),\n",
    "                                            batch_size = bs,\n",
    "                                            class_mode = 'binary')\n",
    "    test_set = test_dt.flow_from_directory('data/dataset/test_set',\n",
    "                                            target_size = (img_width, img_height),\n",
    "                                            batch_size = bs,\n",
    "                                            class_mode = 'binary')\n",
    "    \n",
    "    model = create_model(p = 0.6, input_shape = (img_width, img_height, 3))\n",
    "    model.fit_generator(training_set,\n",
    "                       steps_per_epoch = 8000/bs,\n",
    "                       epochs = epochs,\n",
    "                       validation_data = test_set,\n",
    "                       validation_steps = 2000/bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run run_training(bs = 32, epochs = 100) should take 4 hours approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "##### The idea behind Recurrent Neural Netwoks\n",
    "Before starting to dig deep into Recurrent Neural Networks, here is some of the main use cases for these algorithmes\n",
    "\n",
    "- For Supervised Deep Learning:\n",
    "    1. Artificial Neural Networks are mainly used for Regression and classification\n",
    "    2. Convolutional Neural Networks are used for Computer Vision\n",
    "    3. Recurrent Neural Networks are used for time Series Analysis\n",
    "    \n",
    "    \n",
    "- For Unsepervised Deep learning:\n",
    "    1. Self-Organized Maps are Used for feature detection\n",
    "    2. Deep Boltzmann Machines are Used for Recommendation Systems\n",
    "    3. AutoEncoders are Used for Recommendation Systems\n",
    "    \n",
    "So now let's get back RNN: Deep learning tries to mimic the human brain, in our brain, there are a lot of parts each takes care of a unique task.\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/brain.JPG?raw=true)\n",
    "\n",
    "ANN is like the Temporal Lobe mainly long term memory, CNN represent Occipital Lobe the vision..., RNN is the short term memory otherwise Frontal Lobe\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/RNN.JPG?raw=true)\n",
    "Here are the 4 main types of reinforcement learning:\n",
    "- The first example **One to Many** translates a single input of image into a sentence of text, and the sentence is connected in order for it to make sence\n",
    "- The second example is **Many to One** in this case applied to sentiment analyses from a sentence of text the algorithm can predict either it is a positive or negative feedback\n",
    "- Then there is **Many to Many** in this case used for cross language translation\n",
    "- Finally another example of many to many where the computer produce subtitles for a movie\n",
    "\n",
    "##### The vanishing Gradient Problem\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/RNN2.JPG?raw=true)\n",
    "The further away we try to update our weights to more problem we have,  since each time weights are updated on a certain time the gradient vanishes more and more and the weights can't be updated properlly otherwise in the proper time to properly train the model\n",
    "\n",
    "more details (http://colah.github.io/)\n",
    "##### Long Short-Term Memory Networks (LSTMs)\n",
    "LSTMs are the soltion for the vanishing Gradient Problem:\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/LSTM.JPG?raw=true)\n",
    "The pipe in the top is what solves the problem, this pipeline is called memory cell that can freely travell through time, when we backpropagate we don't have that problem of vanishing gradient.\n",
    "\n",
    "# RNN: LSTM: Predicting Stock Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/3/2012</td>\n",
       "      <td>325.25</td>\n",
       "      <td>332.83</td>\n",
       "      <td>324.97</td>\n",
       "      <td>663.59</td>\n",
       "      <td>7,380,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/4/2012</td>\n",
       "      <td>331.27</td>\n",
       "      <td>333.87</td>\n",
       "      <td>329.08</td>\n",
       "      <td>666.45</td>\n",
       "      <td>5,749,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/5/2012</td>\n",
       "      <td>329.83</td>\n",
       "      <td>330.75</td>\n",
       "      <td>326.89</td>\n",
       "      <td>657.21</td>\n",
       "      <td>6,590,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/6/2012</td>\n",
       "      <td>328.34</td>\n",
       "      <td>328.77</td>\n",
       "      <td>323.68</td>\n",
       "      <td>648.24</td>\n",
       "      <td>5,405,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/9/2012</td>\n",
       "      <td>322.04</td>\n",
       "      <td>322.29</td>\n",
       "      <td>309.46</td>\n",
       "      <td>620.76</td>\n",
       "      <td>11,688,800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date    Open    High     Low   Close      Volume\n",
       "0  1/3/2012  325.25  332.83  324.97  663.59   7,380,500\n",
       "1  1/4/2012  331.27  333.87  329.08  666.45   5,749,400\n",
       "2  1/5/2012  329.83  330.75  326.89  657.21   6,590,300\n",
       "3  1/6/2012  328.34  328.77  323.68  648.24   5,405,900\n",
       "4  1/9/2012  322.04  322.29  309.46  620.76  11,688,800"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing training set\n",
    "training_set = pd.read_csv('data/Google_Stock_Price_Train.csv')\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 325.25],\n",
       "       [ 331.27],\n",
       "       [ 329.83],\n",
       "       ..., \n",
       "       [ 793.7 ],\n",
       "       [ 783.33],\n",
       "       [ 782.75]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = training_set.iloc[:, 1:2].values\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "training_set = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1257, 1)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the inputs and outputs\n",
    "X_train = training_set[0:1257]\n",
    "y_train = training_set[1:1258]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.08581368]],\n",
       "\n",
       "       [[ 0.09701243]],\n",
       "\n",
       "       [[ 0.09433366]],\n",
       "\n",
       "       ..., \n",
       "       [[ 0.95163331]],\n",
       "\n",
       "       [[ 0.95725128]],\n",
       "\n",
       "       [[ 0.93796041]]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reshaping\n",
    "X_train = np.reshape(X_train, (1257, 1, 1))\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initializing the RNN\n",
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding the input layer and the LSTM layer\n",
    "regressor.add(LSTM(units = 4, activation = 'sigmoid',\n",
    "                   input_shape = (None, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding the output layer\n",
    "regressor.add(Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1257/1257 [==============================] - 3s - loss: 0.1138     \n",
      "Epoch 2/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0936     \n",
      "Epoch 3/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0872     \n",
      "Epoch 4/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0847     \n",
      "Epoch 5/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0831     \n",
      "Epoch 6/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0815     \n",
      "Epoch 7/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0798     \n",
      "Epoch 8/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0781     \n",
      "Epoch 9/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0763     \n",
      "Epoch 10/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0745     \n",
      "Epoch 11/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0727     \n",
      "Epoch 12/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0707     \n",
      "Epoch 13/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0687     \n",
      "Epoch 14/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0667     \n",
      "Epoch 15/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0645     \n",
      "Epoch 16/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0625     \n",
      "Epoch 17/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0601     \n",
      "Epoch 18/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0577     \n",
      "Epoch 19/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0553     \n",
      "Epoch 20/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0528     \n",
      "Epoch 21/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0503     \n",
      "Epoch 22/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0476     \n",
      "Epoch 23/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0450     \n",
      "Epoch 24/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0423     \n",
      "Epoch 25/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0395     \n",
      "Epoch 26/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0368     \n",
      "Epoch 27/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0340     \n",
      "Epoch 28/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0314     \n",
      "Epoch 29/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0287     \n",
      "Epoch 30/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0261     \n",
      "Epoch 31/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0235     \n",
      "Epoch 32/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0211     \n",
      "Epoch 33/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0188     \n",
      "Epoch 34/200\n",
      "1257/1257 [==============================] - ETA: 0s - loss: 0.016 - 0s - loss: 0.0166     \n",
      "Epoch 35/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0146     \n",
      "Epoch 36/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0127     \n",
      "Epoch 37/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0109     \n",
      "Epoch 38/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0094     \n",
      "Epoch 39/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0080     \n",
      "Epoch 40/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0067     \n",
      "Epoch 41/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0056     \n",
      "Epoch 42/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0046     \n",
      "Epoch 43/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0038     \n",
      "Epoch 44/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0031     \n",
      "Epoch 45/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0025     \n",
      "Epoch 46/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0020     \n",
      "Epoch 47/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0016     \n",
      "Epoch 48/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0013     \n",
      "Epoch 49/200\n",
      "1257/1257 [==============================] - 0s - loss: 0.0010      \n",
      "Epoch 50/200\n",
      "1257/1257 [==============================] - 0s - loss: 8.4184e-04     \n",
      "Epoch 51/200\n",
      "1257/1257 [==============================] - 0s - loss: 6.8698e-04     \n",
      "Epoch 52/200\n",
      "1257/1257 [==============================] - 0s - loss: 5.6984e-04     \n",
      "Epoch 53/200\n",
      "1257/1257 [==============================] - 0s - loss: 4.8025e-04     \n",
      "Epoch 54/200\n",
      "1257/1257 [==============================] - 0s - loss: 4.1493e-04     \n",
      "Epoch 55/200\n",
      "1257/1257 [==============================] - 0s - loss: 3.6402e-04     \n",
      "Epoch 56/200\n",
      "1257/1257 [==============================] - 0s - loss: 3.2865e-04     \n",
      "Epoch 57/200\n",
      "1257/1257 [==============================] - 0s - loss: 3.0478e-04     \n",
      "Epoch 58/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.8529e-04     \n",
      "Epoch 59/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.7213e-04     \n",
      "Epoch 60/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.6282e-04     \n",
      "Epoch 61/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5770e-04     \n",
      "Epoch 62/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5273e-04     \n",
      "Epoch 63/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5007e-04     \n",
      "Epoch 64/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4745e-04     \n",
      "Epoch 65/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4726e-04     \n",
      "Epoch 66/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4625e-04     \n",
      "Epoch 67/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4580e-04     \n",
      "Epoch 68/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4511e-04     \n",
      "Epoch 69/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4423e-04     \n",
      "Epoch 70/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4535e-04     \n",
      "Epoch 71/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4445e-04     \n",
      "Epoch 72/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4545e-04     \n",
      "Epoch 73/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4417e-04     \n",
      "Epoch 74/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4505e-04     \n",
      "Epoch 75/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4493e-04     \n",
      "Epoch 76/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4521e-04     \n",
      "Epoch 77/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4388e-04     \n",
      "Epoch 78/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4415e-04     \n",
      "Epoch 79/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4462e-04     \n",
      "Epoch 80/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4432e-04     \n",
      "Epoch 81/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4509e-04     \n",
      "Epoch 82/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4609e-04     \n",
      "Epoch 83/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4592e-04     \n",
      "Epoch 84/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4492e-04     \n",
      "Epoch 85/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4560e-04     \n",
      "Epoch 86/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4564e-04     \n",
      "Epoch 87/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4394e-04     \n",
      "Epoch 88/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4525e-04     \n",
      "Epoch 89/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4420e-04     \n",
      "Epoch 90/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4456e-04     \n",
      "Epoch 91/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4425e-04     \n",
      "Epoch 92/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4495e-04     \n",
      "Epoch 93/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4486e-04     \n",
      "Epoch 94/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4429e-04     \n",
      "Epoch 95/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4547e-04     \n",
      "Epoch 96/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4511e-04     \n",
      "Epoch 97/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4480e-04     \n",
      "Epoch 98/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4454e-04     \n",
      "Epoch 99/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4450e-04     \n",
      "Epoch 100/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4521e-04     \n",
      "Epoch 101/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4741e-04     \n",
      "Epoch 102/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4489e-04     \n",
      "Epoch 103/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4543e-04     \n",
      "Epoch 104/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4668e-04     \n",
      "Epoch 105/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4563e-04     \n",
      "Epoch 106/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4721e-04     \n",
      "Epoch 107/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4665e-04     \n",
      "Epoch 108/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4567e-04     \n",
      "Epoch 109/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4540e-04     \n",
      "Epoch 110/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4496e-04     \n",
      "Epoch 111/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4487e-04     \n",
      "Epoch 112/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4576e-04     \n",
      "Epoch 113/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4607e-04     \n",
      "Epoch 114/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4616e-04     \n",
      "Epoch 115/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4771e-04     \n",
      "Epoch 116/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4542e-04     \n",
      "Epoch 117/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4595e-04     \n",
      "Epoch 118/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4746e-04     \n",
      "Epoch 119/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4645e-04     \n",
      "Epoch 120/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4418e-04     \n",
      "Epoch 121/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4600e-04     \n",
      "Epoch 122/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4660e-04     \n",
      "Epoch 123/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4682e-04     \n",
      "Epoch 124/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4646e-04     \n",
      "Epoch 125/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4802e-04     \n",
      "Epoch 126/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4570e-04     \n",
      "Epoch 127/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4588e-04     \n",
      "Epoch 128/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4728e-04     \n",
      "Epoch 129/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4667e-04     \n",
      "Epoch 130/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4536e-04     \n",
      "Epoch 131/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4908e-04     \n",
      "Epoch 132/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4619e-04     \n",
      "Epoch 133/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4867e-04     \n",
      "Epoch 134/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4611e-04     \n",
      "Epoch 135/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4638e-04     \n",
      "Epoch 136/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4701e-04     \n",
      "Epoch 137/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4771e-04     \n",
      "Epoch 138/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4729e-04     \n",
      "Epoch 139/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4623e-04     \n",
      "Epoch 140/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4802e-04     \n",
      "Epoch 141/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4472e-04     \n",
      "Epoch 142/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4725e-04     \n",
      "Epoch 143/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4600e-04     \n",
      "Epoch 144/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4492e-04     \n",
      "Epoch 145/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4506e-04     \n",
      "Epoch 146/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4661e-04     \n",
      "Epoch 147/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4616e-04     \n",
      "Epoch 148/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4524e-04     \n",
      "Epoch 149/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4517e-04     \n",
      "Epoch 150/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4591e-04     \n",
      "Epoch 151/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4557e-04     \n",
      "Epoch 152/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4800e-04     \n",
      "Epoch 153/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4618e-04     \n",
      "Epoch 154/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4726e-04     \n",
      "Epoch 155/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4503e-04     \n",
      "Epoch 156/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4833e-04     \n",
      "Epoch 157/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4545e-04     \n",
      "Epoch 158/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4522e-04     \n",
      "Epoch 159/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4954e-04     \n",
      "Epoch 160/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5030e-04     \n",
      "Epoch 161/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4966e-04     \n",
      "Epoch 162/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4571e-04     \n",
      "Epoch 163/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4516e-04     \n",
      "Epoch 164/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4843e-04     \n",
      "Epoch 165/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4615e-04     \n",
      "Epoch 166/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5215e-04     \n",
      "Epoch 167/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4783e-04     \n",
      "Epoch 168/200\n",
      "1257/1257 [==============================] - ETA: 0s - loss: 2.4311e-0 - 0s - loss: 2.4659e-04     \n",
      "Epoch 169/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4788e-04     \n",
      "Epoch 170/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4655e-04     \n",
      "Epoch 171/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4636e-04     \n",
      "Epoch 172/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4552e-04     \n",
      "Epoch 173/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4615e-04     \n",
      "Epoch 174/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5103e-04     \n",
      "Epoch 175/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4719e-04     \n",
      "Epoch 176/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4629e-04     \n",
      "Epoch 177/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4509e-04     \n",
      "Epoch 178/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4522e-04     \n",
      "Epoch 179/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4612e-04     \n",
      "Epoch 180/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4506e-04     \n",
      "Epoch 181/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4445e-04     \n",
      "Epoch 182/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4457e-04     \n",
      "Epoch 183/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4865e-04     \n",
      "Epoch 184/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4944e-04     \n",
      "Epoch 185/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4751e-04     \n",
      "Epoch 186/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4834e-04     \n",
      "Epoch 187/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4735e-04     \n",
      "Epoch 188/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5173e-04     \n",
      "Epoch 189/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.5016e-04     \n",
      "Epoch 190/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4600e-04     \n",
      "Epoch 191/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4621e-04     \n",
      "Epoch 192/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4599e-04     \n",
      "Epoch 193/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4917e-04     \n",
      "Epoch 194/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1257/1257 [==============================] - 0s - loss: 2.5011e-04     \n",
      "Epoch 195/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4535e-04     \n",
      "Epoch 196/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4560e-04     \n",
      "Epoch 197/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4989e-04     \n",
      "Epoch 198/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4665e-04     \n",
      "Epoch 199/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4517e-04     \n",
      "Epoch 200/200\n",
      "1257/1257 [==============================] - 0s - loss: 2.4980e-04     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xbcb09ab4e0>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the RNN to the training set\n",
    "regressor.fit(X_train, y_train, batch_size = 32, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 778.81],\n",
       "       [ 788.36],\n",
       "       [ 786.08],\n",
       "       [ 795.26],\n",
       "       [ 806.4 ],\n",
       "       [ 807.86],\n",
       "       [ 805.  ],\n",
       "       [ 807.14],\n",
       "       [ 807.48],\n",
       "       [ 807.08],\n",
       "       [ 805.81],\n",
       "       [ 805.12],\n",
       "       [ 806.91],\n",
       "       [ 807.25],\n",
       "       [ 822.3 ],\n",
       "       [ 829.62],\n",
       "       [ 837.81],\n",
       "       [ 834.71],\n",
       "       [ 814.66],\n",
       "       [ 796.86]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the real stock prices\n",
    "test_set = pd.read_csv('data/Google_Stock_Price_Test.csv')\n",
    "real_stock_price = test_set.iloc[:, 1:2].values\n",
    "real_stock_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.92955205],\n",
       "       [ 0.94731751],\n",
       "       [ 0.94307612],\n",
       "       [ 0.96015329],\n",
       "       [ 0.98087655],\n",
       "       [ 0.98359253],\n",
       "       [ 0.97827219],\n",
       "       [ 0.98225314],\n",
       "       [ 0.98288563],\n",
       "       [ 0.98214153],\n",
       "       [ 0.979779  ],\n",
       "       [ 0.97849542],\n",
       "       [ 0.98182528],\n",
       "       [ 0.98245777],\n",
       "       [ 1.01045465],\n",
       "       [ 1.02407173],\n",
       "       [ 1.03930724],\n",
       "       [ 1.03354044],\n",
       "       [ 0.99624228],\n",
       "       [ 0.9631297 ]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = real_stock_price\n",
    "inputs = sc.transform(inputs)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = np.reshape(inputs, (20, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_stock_price = regressor.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 777.93817139],\n",
       "       [ 786.70074463],\n",
       "       [ 784.61047363],\n",
       "       [ 793.02087402],\n",
       "       [ 803.20489502],\n",
       "       [ 804.5378418 ],\n",
       "       [ 801.9263916 ],\n",
       "       [ 803.88049316],\n",
       "       [ 804.19091797],\n",
       "       [ 803.82580566],\n",
       "       [ 802.66601562],\n",
       "       [ 802.03594971],\n",
       "       [ 803.67059326],\n",
       "       [ 803.98095703],\n",
       "       [ 817.63439941],\n",
       "       [ 823.87738037],\n",
       "       [ 830.84533691],\n",
       "       [ 828.21002197],\n",
       "       [ 810.74047852],\n",
       "       [ 794.48504639]], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
    "predicted_stock_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xbcaa6ea780>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAETCAYAAADUAmpRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYMnAGGMMlgyMMcZgycAYYwzw/wEuoIN3LPks3wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbcaecdf940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualising the results\n",
    "%matplotlib inline\n",
    "plt.plot(real_stock_price, color = 'red',\n",
    "         label = 'Real Google Stock Price')\n",
    "plt.plot(predicted_stock_price, color = 'blue',\n",
    "         label = 'Predicted Google Stock Price')\n",
    "plt.title('Google Stock Price Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rsp = pd.read_csv('data/Google_Stock_Price_Train.csv')\n",
    "rsp = rsp.iloc[:, 1:2].values\n",
    "psp = regressor.predict(X_train)\n",
    "psp = sc.inverse_transform(psp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xbcaa7460b8>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAETCAYAAADUAmpRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbcaa45d828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rsp, color = 'red',\n",
    "         label = 'Real Google Stock Price')\n",
    "plt.plot(psp, color = 'blue',\n",
    "         label = 'Predicted Google Stock Price')\n",
    "plt.title('Google Stock Price Predictions')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "--------------------------\n",
    "\n",
    "--------------------------\n",
    "\n",
    "--------------------------\n",
    "\n",
    "--------------------------\n",
    "\n",
    "--------------------------\n",
    "\n",
    "--------------------------\n",
    "\n",
    "# Self Organizing Maps\n",
    "Self organized maps are used for dimentionality reduction, visualization, feature extraction and a lot of other application\n",
    "\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/SOM.JPG?raw=true)\n",
    "SOM works differently than other DL algorithms, In the image the X1, X2, X3 are the inputs and the nodes represent the output, like with other DL algorithms weights are initialzed in the begining with a randome value close to zero, then the for each row the distance between the row and the node is calculated and we record the closest one. One we find the closest one we go back and update the weights for this node to make it even closer to our row, then the all radius around this node will have its weight updated, and the closer is the node to the main node the heavier its weights are going to be updated then its going to fade out... Then we go to row2 and redo the process again. As we go through this process the radius of each node shrinks to influence only the closer points around it\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/SOM2.JPG?raw=true)\n",
    "Some important points about SOM:\n",
    "- SOM retain topology of the input set\n",
    "- SOM reveal correlations that are not easily identified\n",
    "- SOM classify data without supervision\n",
    "- No target vector -> no backpropagation\n",
    "- No lateral connections between output nodes\n",
    "\n",
    "# SOM: Fraud Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A13</th>\n",
       "      <th>A14</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15776156</td>\n",
       "      <td>1</td>\n",
       "      <td>22.08</td>\n",
       "      <td>11.46</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.585</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1213</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15739548</td>\n",
       "      <td>0</td>\n",
       "      <td>22.67</td>\n",
       "      <td>7.00</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15662854</td>\n",
       "      <td>0</td>\n",
       "      <td>29.58</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>280</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15687688</td>\n",
       "      <td>0</td>\n",
       "      <td>21.67</td>\n",
       "      <td>11.50</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15715750</td>\n",
       "      <td>1</td>\n",
       "      <td>20.17</td>\n",
       "      <td>8.17</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1.960</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID  A1     A2     A3  A4  A5  A6     A7  A8  A9  A10  A11  A12  \\\n",
       "0    15776156   1  22.08  11.46   2   4   4  1.585   0   0    0    1    2   \n",
       "1    15739548   0  22.67   7.00   2   8   4  0.165   0   0    0    0    2   \n",
       "2    15662854   0  29.58   1.75   1   4   4  1.250   0   0    0    1    2   \n",
       "3    15687688   0  21.67  11.50   1   5   3  0.000   1   1   11    1    2   \n",
       "4    15715750   1  20.17   8.17   2   6   4  1.960   1   1   14    0    2   \n",
       "\n",
       "   A13   A14  Class  \n",
       "0  100  1213      0  \n",
       "1  160     1      0  \n",
       "2  280     1      0  \n",
       "3    0     1      1  \n",
       "4   60   159      1  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/Credit_Card_Applications.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.42681467e-01,   1.00000000e+00,   1.25263158e-01, ...,\n",
       "          5.00000000e-01,   5.00000000e-02,   1.21200000e-02],\n",
       "       [  6.96090562e-01,   0.00000000e+00,   1.34135338e-01, ...,\n",
       "          5.00000000e-01,   8.00000000e-02,   0.00000000e+00],\n",
       "       [  3.88981656e-01,   0.00000000e+00,   2.38045113e-01, ...,\n",
       "          5.00000000e-01,   1.40000000e-01,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  4.39420332e-01,   0.00000000e+00,   7.63909774e-02, ...,\n",
       "          5.00000000e-01,   5.00000000e-02,   0.00000000e+00],\n",
       "       [  8.44034934e-01,   0.00000000e+00,   2.05563910e-01, ...,\n",
       "          5.00000000e-01,   6.00000000e-02,   1.10000000e-04],\n",
       "       [  1.06907888e-01,   1.00000000e+00,   4.09774436e-01, ...,\n",
       "          0.00000000e+00,   2.80000000e-01,   0.00000000e+00]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "sc.fit_transform(X)\n",
    "X = sc.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training the SOM\n",
    "from minisom import MiniSom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "som = MiniSom(x = 10, y = 10, input_len = 15, sigma = 1.0, learning_rate = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOM Steps:\n",
    "1. We start with a dataset of n features independent variables\n",
    "2. We create a grid composed of nodes, each one having a weight vector of n features elements\n",
    "3. Randomly initialize the values of the weight vectors to small numbers close to 0\n",
    "4. Select one random observation point from the dataset\n",
    "5. Compute the euclidean distances from this point to the different neurons in the network\n",
    "6. Select the neuron that has the minimum distance to the point. This neuron is called the winning node\n",
    "7. Update the weights of the winning node to move it closer to the point\n",
    "8. Using Gaussian neighbourhood function of mean the winning node, also update the weights of the winning node neighbours to move them closer to the point. The neighbourhoud radius is the sigma in the Gaussian function\n",
    "9. Repeat step 1-5 and update the weights after each observation or after a batch of observations, until the network converges to a point where the neighbourhood stops decreasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the weights\n",
    "som.random_weights_init(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Start the training \"Step 4-9\"\n",
    "som.train_random(data = X, num_iteration = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualizing the results\n",
    "from pylab import bone, pcolor, colorbar, plot, show\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD7CAYAAADJukfwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbcb9623dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bone()\n",
    "pcolor(som.distance_map().T)\n",
    "colorbar()\n",
    "markers = ['o', 's']\n",
    "colors = ['r', 'g']\n",
    "for i, x in enumerate(X):\n",
    "    w = som.winner(x)\n",
    "    plot(w[0] +0.5, w[1] + 0.5,  markers[y[i]],\n",
    "         markeredgecolor = colors[y[i]],\n",
    "         markerfacecolor = 'None',\n",
    "         markersize = 5,\n",
    "         markeredgewidth = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finding the frauders\n",
    "mappings = som.win_map(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frauds = mappings[(6, 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frauds = sc.inverse_transform(frauds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 15)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frauds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "--------------------------\n",
    "\n",
    "--------------------------\n",
    "\n",
    "--------------------------\n",
    "\n",
    "--------------------------\n",
    "\n",
    "\n",
    "# Boltzmann Machines\n",
    "\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/DL.JPG?raw=true)\n",
    "\n",
    "In the top we have the 4 algorithms that weve seen up till now, so the first one, is the Artificial Neural Network, the second one is the Convolutional Neural Network, then there is the Recurrent Neural Network ,and finally the self organizing maps. One common point between these 4 algorithms is that they are all directional there is a direction in which the algorithm flows\n",
    "\n",
    "The Boltzman Machines breaks this directionality rules in the bottom of the image we have a preview of what this algorithms looks like, the first remark is that there is no output layer!, there is just the hidden layers in blue and the visible layers in white. A seonc dremark is that everything is connected to everything. the input nodes are connected between them one reason is that for Boltzman there is no difference between a hidden node and a visible node, everything is part of all, and Boltzman machine can also generate data not only understand it. The only difference is that Visile node is things that we can measure while we can't measure the hidden ndes.\n",
    "\n",
    "The one problem with Boltzman Machines is that as we increase the number of nodes we increase exponentielly the complexity of the model, and at a certain point we just can't afford to do all the calculations.... so a solution to tha problem is\n",
    "\n",
    "##### Restricted Boltzmann Machines\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/RBM.JPG?raw=true)\n",
    "\n",
    "## Boltzmann Recommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing the movies dataset\n",
    "movies = pd.read_csv('data/ml-1m/movies.dat', sep = '::',\n",
    "                     header = None,  engine = 'python', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                   1                             2\n",
       "0  1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1  2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2  3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3  4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4  5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6035</th>\n",
       "      <td>6036</td>\n",
       "      <td>F</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>32603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6036</th>\n",
       "      <td>6037</td>\n",
       "      <td>F</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>76006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6037</th>\n",
       "      <td>6038</td>\n",
       "      <td>F</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>14706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6038</th>\n",
       "      <td>6039</td>\n",
       "      <td>F</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>01060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6039</th>\n",
       "      <td>6040</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>11106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1   2   3      4\n",
       "6035  6036  F  25  15  32603\n",
       "6036  6037  F  45   1  76006\n",
       "6037  6038  F  56   1  14706\n",
       "6038  6039  F  45   0  01060\n",
       "6039  6040  M  25   6  11106"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the users dataset\n",
    "users = pd.read_csv('data/ml-1m/users.dat', sep = '::',\n",
    "                     header = None,  engine = 'python', encoding = 'latin-1')\n",
    "users.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000204</th>\n",
       "      <td>6040</td>\n",
       "      <td>1091</td>\n",
       "      <td>1</td>\n",
       "      <td>956716541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000205</th>\n",
       "      <td>6040</td>\n",
       "      <td>1094</td>\n",
       "      <td>5</td>\n",
       "      <td>956704887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000206</th>\n",
       "      <td>6040</td>\n",
       "      <td>562</td>\n",
       "      <td>5</td>\n",
       "      <td>956704746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000207</th>\n",
       "      <td>6040</td>\n",
       "      <td>1096</td>\n",
       "      <td>4</td>\n",
       "      <td>956715648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000208</th>\n",
       "      <td>6040</td>\n",
       "      <td>1097</td>\n",
       "      <td>4</td>\n",
       "      <td>956715569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0     1  2          3\n",
       "1000204  6040  1091  1  956716541\n",
       "1000205  6040  1094  5  956704887\n",
       "1000206  6040   562  5  956704746\n",
       "1000207  6040  1096  4  956715648\n",
       "1000208  6040  1097  4  956715569"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the ratings dataset\n",
    "ratings = pd.read_csv('data/ml-1m/ratings.dat', sep = '::',\n",
    "                     header = None,  engine = 'python', encoding = 'latin-1')\n",
    "ratings.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>1.1</th>\n",
       "      <th>5</th>\n",
       "      <th>874965758</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79994</th>\n",
       "      <td>943</td>\n",
       "      <td>1067</td>\n",
       "      <td>2</td>\n",
       "      <td>875501756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>943</td>\n",
       "      <td>1074</td>\n",
       "      <td>4</td>\n",
       "      <td>888640250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>943</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>888640250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>943</td>\n",
       "      <td>1228</td>\n",
       "      <td>3</td>\n",
       "      <td>888640275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>943</td>\n",
       "      <td>1330</td>\n",
       "      <td>3</td>\n",
       "      <td>888692465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         1   1.1  5  874965758\n",
       "79994  943  1067  2  875501756\n",
       "79995  943  1074  4  888640250\n",
       "79996  943  1188  3  888640250\n",
       "79997  943  1228  3  888640275\n",
       "79998  943  1330  3  888692465"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first column correspond the the userId, the second column the movie\n",
    "#id, the third column corresponds to the rating,  the last is timestamp\n",
    "#Preparing the training set and test set\n",
    "#we're going to use the 100k dataset not the 1m to make it run quicker\n",
    "training_set = pd.read_csv('data/ml-100k/u1.base', delimiter = '\\t')\n",
    "training_set.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79999, 4)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting to an array and get rid of the unnecesary columns\n",
    "training_set = np.array(training_set, dtype = 'int')\n",
    "training_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19999, 4)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = pd.read_csv('data/ml-100k/u1.test', delimiter = '\\t')\n",
    "test_set = np.array(test_set, dtype = 'int')\n",
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "943"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the number of users\n",
    "nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))\n",
    "nb_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1682"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))\n",
    "nb_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting the data into an array with users in lines and movies in columns\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        id_movies = data[:, 1][data[:, 0] == id_users]\n",
    "        id_ratings = data[:, 2][data[:, 0] == id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 1.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 1.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 2.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 3.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.0,\n",
       " 2.0,\n",
       " 5.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 4.0,\n",
       " 4.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 2.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now our users are row indices, and the columns indices are the movies\n",
    "#ids and the row elements are the ratings\n",
    "training_set = convert(training_set)\n",
    "training_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the data into Tors tensors\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    0     3     4  ...      0     0     0\n",
       "    4     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "       ...          ⋱          ...       \n",
       "    5     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "    0     5     0  ...      0     0     0\n",
       "[torch.FloatTensor of size 943x1682]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting the ratings into binary ratings 1(liked) or 0 (Not Liked)\n",
    "training_set[training_set == 0] = -1\n",
    "training_set[training_set == 1] = 0\n",
    "training_set[training_set == 2] = 0\n",
    "training_set[training_set >= 3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   -1     1     1  ...     -1    -1    -1\n",
       "    1    -1    -1  ...     -1    -1    -1\n",
       "   -1    -1    -1  ...     -1    -1    -1\n",
       "       ...          ⋱          ...       \n",
       "    1    -1    -1  ...     -1    -1    -1\n",
       "   -1    -1    -1  ...     -1    -1    -1\n",
       "   -1     1    -1  ...     -1    -1    -1\n",
       "[torch.FloatTensor of size 943x1682]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set[test_set == 0] = -1\n",
    "test_set[test_set == 1] = 0\n",
    "test_set[test_set == 2] = 0\n",
    "test_set[test_set >= 3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the architecture of the Restricted Boltzmann Machine\n",
    "class RBM():\n",
    "    # nv: number visible nodes, nh: number of hidden nodes\n",
    "    def __init__(self, nv, nh):\n",
    "        self.W = torch.randn(nh, nv)  # weights in a matrix nh x nv\n",
    "        self.a = torch.randn(1, nh)   #bias for hidden nodeinitialization\n",
    "        self.b = torch.randn(1, nv)   #bias for hidden nodeinitialization\n",
    "    \n",
    "    #sampling the hidden nodes according to the Phn/vn\n",
    "    #x: the vector of visible neurons\n",
    "    def sample_h(self, x):\n",
    "        #prob of h given v\n",
    "        wx = torch.mm(x, self.W.t())\n",
    "        activation = wx + self.a.expand_as(wx)\n",
    "        p_h_given_v = torch.sigmoid(activation)\n",
    "        #returns the probability and sampling of the hidden neurons given the visible neurons\n",
    "        return p_h_given_v, torch.bernoulli(p_h_given_v) \n",
    "    \n",
    "    #returns the probabilities that the visible nodes = 1 given the prob of the hidden nodes\n",
    "    def sample_v(self, y):\n",
    "        #prob of h given v\n",
    "        wy = torch.mm(y, self.W)\n",
    "        activation = wy + self.b.expand_as(wy)\n",
    "        p_v_given_h = torch.sigmoid(activation)\n",
    "        return p_v_given_h, torch.bernoulli(p_v_given_h)\n",
    "    \n",
    "    #Contrastive divergence: approximating the Log-Likelihood Gradient\n",
    "    #v0: input vector, vk: visible node after k sampling\n",
    "    #ph0: vector of probs of h...., phk: the probs of hidden nodes after k sampling\n",
    "    def train(self, v0, vk, ph0, phk):\n",
    "        self.W += torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)\n",
    "        self.b += torch.sum((v0 - vk), 0)\n",
    "        self.a += torch.sum((ph0 - phk), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nv = len(training_set[0])\n",
    "nh = 100\n",
    "batch_size = 100\n",
    "rbm = RBM(nv, nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 0.25038298400148806\n",
      "epoch: 2 loss: 0.2507097316550295\n",
      "epoch: 3 loss: 0.25009039101444547\n",
      "epoch: 4 loss: 0.2500596827355213\n",
      "epoch: 5 loss: 0.2496840671976199\n",
      "epoch: 6 loss: 0.2504718481252974\n",
      "epoch: 7 loss: 0.25015995949413916\n",
      "epoch: 8 loss: 0.24793178495591267\n",
      "epoch: 9 loss: 0.25014345146785155\n",
      "epoch: 10 loss: 0.2494001175279076\n"
     ]
    }
   ],
   "source": [
    "#Training the RBM\n",
    "nb_epoch = 10\n",
    "\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    \n",
    "    for id_user in range(0, nb_users - batch_size, batch_size):\n",
    "        vk = training_set[id_user : id_user+batch_size]\n",
    "        v0 = training_set[id_user : id_user+batch_size]\n",
    "        ph0,_ = rbm.sample_h(v0)\n",
    "        \n",
    "        for k in range(10):\n",
    "            _,hk = rbm.sample_h(vk)\n",
    "            _,vk = rbm.sample_v(hk)\n",
    "            vk[v0 < 0] = v0[v0 < 0]\n",
    "            \n",
    "        phk,_ = rbm.sample_h(vk)\n",
    "        rbm.train(v0, vk, ph0, phk)\n",
    "        train_loss += torch.mean(torch.abs(v0[v0 >= 0] - vk[v0 >= 0]))\n",
    "        s += 1.\n",
    "    print('epoch: '+ str(epoch)+' loss: '+ str(train_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.24402266619620303\n"
     ]
    }
   ],
   "source": [
    "# Testing the RBM\n",
    "test_loss = 0\n",
    "s = 0.\n",
    "\n",
    "for id_user in range(nb_users):\n",
    "    v = training_set[id_user : id_user+1]\n",
    "    vt = test_set[id_user : id_user+1]\n",
    "    \n",
    "    if len(vt[vt >= 0]) > 0:\n",
    "        _,h = rbm.sample_h(v)\n",
    "        _,v = rbm.sample_v(h)\n",
    "        test_loss += torch.mean(torch.abs(vt[vt >= 0] - v[vt >= 0]))\n",
    "        s += 1.\n",
    "print('test loss: '+str(test_loss/s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "\n",
    "--------------------------\n",
    "\n",
    "--------------------------\n",
    "\n",
    "--------------------------\n",
    "\n",
    "--------------------------\n",
    "\n",
    "--------------------------\n",
    "\n",
    "\n",
    "# Auto encoders\n",
    "![](https://github.com/sangloo/Data-Science-Learning-Path/blob/master/JupyterProjects%20'WIP'/data/imgs/AutoEncoder.JPG?raw=true)\n",
    "We're back to the directed type of Neural Network, everything is moving from left to right. An AutoEncoder encode itself like explained in the image above. \n",
    "\n",
    "Steps for training an Autoencoder\n",
    "1. We start an array where the lines (observations) correspond to the users and the columns (features) correspond to the movies. Each cell (u, i) contains the ratings from (1 to 5, 0 if no rating) of the movie i by the user u\n",
    "2. The first user goes into the network, the input vector X = (r1, r2, ..., rn) contains all its ratings for all movies\n",
    "3. The input vector X is encoded into a vector Z of lower dimensions by mapping function f (eg: sigmoid) z = f(Wx + b)  where W is the vector of input weights and b the bias\n",
    "4. z is then encoded into the output vector y of same dimensions as x, aiming to replicate the input vector x.\n",
    "5. the reconstruction error d(x, y) = ||x - y|| is computed, the goal is to minimize it\n",
    "6. Back propagation: from right to left, the error is back-propagated the weights are updated according to how much they are responsible for the error. The learning rate decides by how much we update the weights\n",
    "7. Repeat steps 1 to 6 and update the weights after each observation or after a batch of observations\n",
    "8. When the whole training set passed through the ANN, that makes an epoch, redo more epochs.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AutoEncoders\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "movies = pd.read_csv('data/ml-1m/movies.dat', sep = '::',\n",
    "                     header = None, engine = 'python',\n",
    "                     encoding = 'latin-1')\n",
    "\n",
    "users = pd.read_csv('data/ml-1m/users.dat', sep = '::',\n",
    "                    header = None, engine = 'python',\n",
    "                    encoding = 'latin-1')\n",
    "\n",
    "ratings = pd.read_csv('data/ml-1m/ratings.dat',\n",
    "                      sep = '::', header = None,\n",
    "                      engine = 'python', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparing the training set and the test set\n",
    "training_set = pd.read_csv('data/ml-100k/u1.base', delimiter = '\\t')\n",
    "training_set = np.array(training_set, dtype = 'int')\n",
    "\n",
    "test_set = pd.read_csv('data/ml-100k/u1.test', delimiter = '\\t')\n",
    "test_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the number of users and movies\n",
    "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
    "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting the data into an array with users in lines and movies in columns\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    \n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        id_movies = data[:,1][data[:,0] == id_users]\n",
    "        id_ratings = data[:,2][data[:,0] == id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "        \n",
    "    return new_data\n",
    "\n",
    "\n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting the data into Torch tensors\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the architecture of the Neural Network\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 1.77134057408\n",
      "epoch: 2 loss: 1.09666301483\n",
      "epoch: 3 loss: 1.05321960826\n",
      "epoch: 4 loss: 1.03832902858\n",
      "epoch: 5 loss: 1.03094201223\n",
      "epoch: 6 loss: 1.02650416034\n",
      "epoch: 7 loss: 1.02381507439\n",
      "epoch: 8 loss: 1.02186576999\n",
      "epoch: 9 loss: 1.02082806615\n",
      "epoch: 10 loss: 1.01959024661\n",
      "epoch: 11 loss: 1.01901971359\n",
      "epoch: 12 loss: 1.01798287682\n",
      "epoch: 13 loss: 1.01803009298\n",
      "epoch: 14 loss: 1.01739877257\n",
      "epoch: 15 loss: 1.01714556058\n",
      "epoch: 16 loss: 1.0168889502\n",
      "epoch: 17 loss: 1.01674745965\n",
      "epoch: 18 loss: 1.0163917196\n",
      "epoch: 19 loss: 1.01632532283\n",
      "epoch: 20 loss: 1.01601738727\n",
      "epoch: 21 loss: 1.01589680432\n",
      "epoch: 22 loss: 1.01600166338\n",
      "epoch: 23 loss: 1.01577543918\n",
      "epoch: 24 loss: 1.01571816949\n",
      "epoch: 25 loss: 1.01552507354\n",
      "epoch: 26 loss: 1.01557290665\n",
      "epoch: 27 loss: 1.01550728665\n",
      "epoch: 28 loss: 1.01513467223\n",
      "epoch: 29 loss: 1.0130092321\n",
      "epoch: 30 loss: 1.01133773891\n",
      "epoch: 31 loss: 1.01006339412\n",
      "epoch: 32 loss: 1.00683729534\n",
      "epoch: 33 loss: 1.00685186445\n",
      "epoch: 34 loss: 1.00212324459\n",
      "epoch: 35 loss: 1.00081875795\n",
      "epoch: 36 loss: 0.997482557028\n",
      "epoch: 37 loss: 0.997499936275\n",
      "epoch: 38 loss: 0.99441769281\n",
      "epoch: 39 loss: 0.992735703948\n",
      "epoch: 40 loss: 0.991720861696\n",
      "epoch: 41 loss: 0.990052957852\n",
      "epoch: 42 loss: 0.986331753638\n",
      "epoch: 43 loss: 0.987055858927\n",
      "epoch: 44 loss: 0.985015336804\n",
      "epoch: 45 loss: 0.980925990735\n",
      "epoch: 46 loss: 0.979550078996\n",
      "epoch: 47 loss: 0.978999393358\n",
      "epoch: 48 loss: 0.977158362783\n",
      "epoch: 49 loss: 0.983308518339\n",
      "epoch: 50 loss: 0.976466077938\n",
      "epoch: 51 loss: 0.974284525396\n",
      "epoch: 52 loss: 0.969570312266\n",
      "epoch: 53 loss: 0.974089992596\n",
      "epoch: 54 loss: 0.976478522823\n",
      "epoch: 55 loss: 0.974711723231\n",
      "epoch: 56 loss: 0.969237192792\n",
      "epoch: 57 loss: 0.971541758105\n",
      "epoch: 58 loss: 0.969681995357\n",
      "epoch: 59 loss: 0.96547276471\n",
      "epoch: 60 loss: 0.966872890467\n",
      "epoch: 61 loss: 0.967894559434\n",
      "epoch: 62 loss: 0.961355867663\n",
      "epoch: 63 loss: 0.962384917852\n",
      "epoch: 64 loss: 0.959277207482\n",
      "epoch: 65 loss: 0.957325194237\n",
      "epoch: 66 loss: 0.95568543277\n",
      "epoch: 67 loss: 0.954924956612\n",
      "epoch: 68 loss: 0.964778238751\n",
      "epoch: 69 loss: 0.979245251116\n",
      "epoch: 70 loss: 0.968594612965\n",
      "epoch: 71 loss: 0.966277183836\n",
      "epoch: 72 loss: 0.976965640966\n",
      "epoch: 73 loss: 0.967804771982\n",
      "epoch: 74 loss: 0.967624640418\n",
      "epoch: 75 loss: 0.966104809232\n",
      "epoch: 76 loss: 0.964047852806\n",
      "epoch: 77 loss: 0.961620056822\n",
      "epoch: 78 loss: 0.961085908862\n",
      "epoch: 79 loss: 0.962137450388\n",
      "epoch: 80 loss: 0.962251136654\n",
      "epoch: 81 loss: 0.960452677648\n",
      "epoch: 82 loss: 0.955376665159\n",
      "epoch: 83 loss: 0.953305636343\n",
      "epoch: 84 loss: 0.954050111091\n",
      "epoch: 85 loss: 0.953886474941\n",
      "epoch: 86 loss: 0.951026472774\n",
      "epoch: 87 loss: 0.951729917188\n",
      "epoch: 88 loss: 0.947566101421\n",
      "epoch: 89 loss: 0.949384272465\n",
      "epoch: 90 loss: 0.949853330586\n",
      "epoch: 91 loss: 0.947077926873\n",
      "epoch: 92 loss: 0.944016891282\n",
      "epoch: 93 loss: 0.94603790876\n",
      "epoch: 94 loss: 0.941962451338\n",
      "epoch: 95 loss: 0.942626400143\n",
      "epoch: 96 loss: 0.940341383396\n",
      "epoch: 97 loss: 0.940648748219\n",
      "epoch: 98 loss: 0.938655307591\n",
      "epoch: 99 loss: 0.939528936422\n",
      "epoch: 100 loss: 0.937294042353\n",
      "epoch: 101 loss: 0.938636018048\n",
      "epoch: 102 loss: 0.93629937396\n",
      "epoch: 103 loss: 0.937008082969\n",
      "epoch: 104 loss: 0.935394057735\n",
      "epoch: 105 loss: 0.935978548683\n",
      "epoch: 106 loss: 0.934740917245\n",
      "epoch: 107 loss: 0.935290082002\n",
      "epoch: 108 loss: 0.934307222664\n",
      "epoch: 109 loss: 0.935007149412\n",
      "epoch: 110 loss: 0.933994754668\n",
      "epoch: 111 loss: 0.934176609911\n",
      "epoch: 112 loss: 0.932955134852\n",
      "epoch: 113 loss: 0.933373054115\n",
      "epoch: 114 loss: 0.931645995459\n",
      "epoch: 115 loss: 0.932561884813\n",
      "epoch: 116 loss: 0.931889220798\n",
      "epoch: 117 loss: 0.931383930562\n",
      "epoch: 118 loss: 0.929729137949\n",
      "epoch: 119 loss: 0.930525005936\n",
      "epoch: 120 loss: 0.930172811784\n",
      "epoch: 121 loss: 0.929975480049\n",
      "epoch: 122 loss: 0.930273296547\n",
      "epoch: 123 loss: 0.929143234816\n",
      "epoch: 124 loss: 0.928661561005\n",
      "epoch: 125 loss: 0.928666107565\n",
      "epoch: 126 loss: 0.927782815005\n",
      "epoch: 127 loss: 0.927542112239\n",
      "epoch: 128 loss: 0.927313168781\n",
      "epoch: 129 loss: 0.927027609386\n",
      "epoch: 130 loss: 0.926589555146\n",
      "epoch: 131 loss: 0.926212509754\n",
      "epoch: 132 loss: 0.925398978245\n",
      "epoch: 133 loss: 0.925444427039\n",
      "epoch: 134 loss: 0.925640413221\n",
      "epoch: 135 loss: 0.925282667404\n",
      "epoch: 136 loss: 0.925103489829\n",
      "epoch: 137 loss: 0.92505965454\n",
      "epoch: 138 loss: 0.92429341437\n",
      "epoch: 139 loss: 0.924356080613\n",
      "epoch: 140 loss: 0.923767904184\n",
      "epoch: 141 loss: 0.923998871388\n",
      "epoch: 142 loss: 0.923171176541\n",
      "epoch: 143 loss: 0.923215621822\n",
      "epoch: 144 loss: 0.922759360649\n",
      "epoch: 145 loss: 0.92282010467\n",
      "epoch: 146 loss: 0.921836257945\n",
      "epoch: 147 loss: 0.922235114303\n",
      "epoch: 148 loss: 0.921542265528\n",
      "epoch: 149 loss: 0.921437865681\n",
      "epoch: 150 loss: 0.921277534511\n",
      "epoch: 151 loss: 0.920927588646\n",
      "epoch: 152 loss: 0.921066040945\n",
      "epoch: 153 loss: 0.921552773962\n",
      "epoch: 154 loss: 0.920330169016\n",
      "epoch: 155 loss: 0.920708589264\n",
      "epoch: 156 loss: 0.919877106587\n",
      "epoch: 157 loss: 0.92058249025\n",
      "epoch: 158 loss: 0.91892719993\n",
      "epoch: 159 loss: 0.919765592964\n",
      "epoch: 160 loss: 0.918885631875\n",
      "epoch: 161 loss: 0.919545567713\n",
      "epoch: 162 loss: 0.918779781898\n",
      "epoch: 163 loss: 0.918823763269\n",
      "epoch: 164 loss: 0.918481739634\n",
      "epoch: 165 loss: 0.918405695881\n",
      "epoch: 166 loss: 0.917764113893\n",
      "epoch: 167 loss: 0.918259208535\n",
      "epoch: 168 loss: 0.917696481904\n",
      "epoch: 169 loss: 0.917898945623\n",
      "epoch: 170 loss: 0.917283716155\n",
      "epoch: 171 loss: 0.917386641376\n",
      "epoch: 172 loss: 0.916970946882\n",
      "epoch: 173 loss: 0.917222514188\n",
      "epoch: 174 loss: 0.916661329389\n",
      "epoch: 175 loss: 0.917213425382\n",
      "epoch: 176 loss: 0.916528609031\n",
      "epoch: 177 loss: 0.916726591423\n",
      "epoch: 178 loss: 0.915985944372\n",
      "epoch: 179 loss: 0.916501084557\n",
      "epoch: 180 loss: 0.915956045315\n",
      "epoch: 181 loss: 0.916206773287\n",
      "epoch: 182 loss: 0.915492081746\n",
      "epoch: 183 loss: 0.915664380008\n",
      "epoch: 184 loss: 0.914939041586\n",
      "epoch: 185 loss: 0.915598478606\n",
      "epoch: 186 loss: 0.915022466963\n",
      "epoch: 187 loss: 0.915249513163\n",
      "epoch: 188 loss: 0.915028098788\n",
      "epoch: 189 loss: 0.915383082665\n",
      "epoch: 190 loss: 0.914666699692\n",
      "epoch: 191 loss: 0.914428698916\n",
      "epoch: 192 loss: 0.913897246339\n",
      "epoch: 193 loss: 0.914058756051\n",
      "epoch: 194 loss: 0.914019388948\n",
      "epoch: 195 loss: 0.914218365894\n",
      "epoch: 196 loss: 0.913925522344\n",
      "epoch: 197 loss: 0.91428262049\n",
      "epoch: 198 loss: 0.914424677204\n",
      "epoch: 199 loss: 0.913798710734\n",
      "epoch: 200 loss: 0.913196684593\n"
     ]
    }
   ],
   "source": [
    "# Training the SAE\n",
    "nb_epoch = 200\n",
    "\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    \n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        \n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae(input)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.data[0]*mean_corrector)\n",
    "            s += 1.\n",
    "            optimizer.step()\n",
    "            \n",
    "    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.949942540912\n"
     ]
    }
   ],
   "source": [
    "# Testing the SAE\n",
    "test_loss = 0\n",
    "s = 0.\n",
    "\n",
    "for id_user in range(nb_users):\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    target = Variable(test_set[id_user])\n",
    "    \n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = sae(input)\n",
    "        target.require_grad = False\n",
    "        output[target == 0] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.data[0]*mean_corrector)\n",
    "        s += 1.\n",
    "        \n",
    "print('test loss: '+str(test_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
